{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-1. Linear Regression의 cost 최소화 알고리즘의 원리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hypothesis and cost**  \n",
    "$$H(x)=Wx+b$$ \n",
    "$$cost(W,b) = {1\\over m}\\sum_{i=1}^m(H(x^{(i)})-y^{(i)})^2$$\n",
    "\n",
    "\n",
    "**Simplified hypothesis**  \n",
    "$$H(x)=Wx$$ \n",
    "$$cost(W,b) = {1\\over m}\\sum_{i=1}^m(Wx^{(i)}-y^{(i)})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient descent algorithm(경사 하강)**  \n",
    "1. 개념\n",
    "    - Minimize cost function  \n",
    "    - Gradient descent is used many minimization problems  \n",
    "    - For a given cost function, cost(W,b), it will find W,b to minimize cost  \n",
    "    - It can be applied to more general function: cost(w1,w2,...)\n",
    "\n",
    "\n",
    "2. How it works?  \n",
    "    - Start with initial guesses  \n",
    "        - Start at 0,0 (or any other value)  \n",
    "        - Keeping changing W and b a little bit to try and reduce cost(W,b)\n",
    "    - Each time you change the parameters, you select the gradient which reduces cost(W,b) the most possible  \n",
    "    - Repeat  \n",
    "    - Do so until you converge to a local minimum  \n",
    "    - Has an interesting property  \n",
    "        - Where you start can determine which minimum you end up  \n",
    "        \n",
    "        \n",
    "=> <u>어디서 출발하든 항상 최소점에 도달할 수 있다.</u>(장점, but 예외는 있음)  \n",
    "\n",
    "3. 경사도  \n",
    "    - 미분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Formal definition**\n",
    "$$cost(W,b) = {1\\over m}\\sum_{i=1}^m(Wx^{(i)}-y^{(i)})^2$$\n",
    "=>\n",
    "$$cost(W,b) = {1\\over 2m}\\sum_{i=1}^m(Wx^{(i)}-y^{(i)})^2$$\n",
    "=>\n",
    "$$W := W - \\alpha{\\delta \\over \\delta W}cost(W)$$  \n",
    "<center>(미분 : 한 점에서 기울기)</center>  \n",
    "=>\n",
    "$$W := W - \\alpha{\\delta \\over \\delta W}{1\\over 2m}\\sum_{i=1}^m(Wx^{(i)}-y^{(i)})^2$$\n",
    "=>\n",
    "$$W := W - \\alpha{1\\over 2m}\\sum_{i=1}^m2(Wx^{(i)}-y^{(i)})x^{(i)}$$\n",
    "=>\n",
    "$$W := W - \\alpha{1\\over m}\\sum_{i=1}^m(Wx^{(i)}-y^{(i)})x^{(i)}$$\n",
    "\n",
    "**Gradient descent algorithm**\n",
    "$$W := W - \\alpha{1\\over m}\\sum_{i=1}^m(Wx^{(i)}-y^{(i)})x^{(i)}$$\n",
    "\n",
    "- <u>cost function이 convex function이여야 gradient descent algorithm 사용 가능</u>\n",
    "\n",
    "\n",
    "\n",
    "<img src='https://qph.fs.quoracdn.net/main-qimg-2f62803cd5ba745b6ae51aae6e6c165e.webp'>\n",
    "<center> 그림: convex function </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2. Linear Regression 의 cost 최소화의 TensorFlow 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = [1,2,3]\n",
    "Y = [1,2,3]\n",
    "\n",
    "W = tf.placeholder(tf.float32) \n",
    "\n",
    "# Our hypothesis for linear model X * W\n",
    "hypothesis = X * W # features들을 가로로 놓기 위해\n",
    "\n",
    "# cost/Loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Variables for plotting cost function\n",
    "W_history = []\n",
    "cost_history = []\n",
    "\n",
    "# Launch the graph in a session.\n",
    "with tf.Session() as sess:\n",
    "    for i in range(-30, 50):\n",
    "        curr_W = i * 0.1 # 대략 -3 ~ 5로 0.1 간격으로 움직이겠다\n",
    "        curr_cost = sess.run(cost, feed_dict={W: curr_W})\n",
    "\n",
    "        W_history.append(curr_W)\n",
    "        cost_history.append(curr_cost)\n",
    "\n",
    "# Show the cost function\n",
    "plt.plot(W_history, cost_history)\n",
    "plt.show() # y축 : cost(W), x축 : W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- convex function  \n",
    "- 최적화하는 값은 대략 1  \n",
    "- 최적화하는 값을 찾기 위해 미분을 이용(Gradient descent 이용)  \n",
    "- 1을 기준으로 오른쪽 기울기는 +, 왼쪽은 -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6.8665643 [0.3530584]\n",
      "1 1.9531559 [0.65496445]\n",
      "2 0.55556446 [0.81598103]\n",
      "3 0.15802719 [0.90185654]\n",
      "4 0.04494996 [0.9476568]\n",
      "5 0.012785784 [0.9720836]\n",
      "6 0.003636841 [0.98511124]\n",
      "7 0.0010344847 [0.99205935]\n",
      "8 0.00029424974 [0.995765]\n",
      "9 8.369887e-05 [0.99774134]\n",
      "10 2.3806637e-05 [0.9987954]\n",
      "11 6.772017e-06 [0.9993576]\n",
      "12 1.9260958e-06 [0.9996574]\n",
      "13 5.477728e-07 [0.99981725]\n",
      "14 1.5589518e-07 [0.99990255]\n",
      "15 4.4308663e-08 [0.999948]\n",
      "16 1.2606658e-08 [0.9999723]\n",
      "17 3.5881709e-09 [0.9999852]\n",
      "18 1.0196951e-09 [0.99999213]\n",
      "19 2.8887825e-10 [0.9999958]\n",
      "20 8.02487e-11 [0.99999774]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "x_data = [1, 2, 3]\n",
    "y_data = [1, 2, 3]\n",
    "\n",
    "# Try to find values for W and b to compute y_data = W * x_data\n",
    "# We know that W should be 1\n",
    "# But let's use TensorFlow to figure it out\n",
    "W = tf.Variable(tf.random_normal([1]), name=\"weight\")\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# Our hypothesis for linear model X * W\n",
    "hypothesis = X * W\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize: Gradient Descent using derivative: W -= learning_rate * derivative\n",
    "learning_rate = 0.1\n",
    "gradient = tf.reduce_mean((W * X - Y) * X)\n",
    "descent = W - learning_rate * gradient\n",
    "update = W.assign(descent)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "with tf.Session() as sess:\n",
    "    # Initializes global variables in the graph.\n",
    "    sess.run(tf.global_variables_initializer()) # global_variables : 전역 변수\n",
    "\n",
    "    for step in range(21):\n",
    "        _, cost_val, W_val = sess.run(\n",
    "            [update, cost, W], feed_dict={X: x_data, Y: y_data}\n",
    "        )\n",
    "        print(step, cost_val, W_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.2666664\n",
      "1 1.0177778\n",
      "2 1.0011852\n",
      "3 1.000079\n",
      "4 1.0000052\n",
      "5 1.0000004\n",
      "6 1.0\n",
      "7 1.0\n",
      "8 1.0\n",
      "9 1.0\n",
      "10 1.0\n",
      "11 1.0\n",
      "12 1.0\n",
      "13 1.0\n",
      "14 1.0\n",
      "15 1.0\n",
      "16 1.0\n",
      "17 1.0\n",
      "18 1.0\n",
      "19 1.0\n",
      "20 1.0\n",
      "21 1.0\n",
      "22 1.0\n",
      "23 1.0\n",
      "24 1.0\n",
      "25 1.0\n",
      "26 1.0\n",
      "27 1.0\n",
      "28 1.0\n",
      "29 1.0\n",
      "30 1.0\n",
      "31 1.0\n",
      "32 1.0\n",
      "33 1.0\n",
      "34 1.0\n",
      "35 1.0\n",
      "36 1.0\n",
      "37 1.0\n",
      "38 1.0\n",
      "39 1.0\n",
      "40 1.0\n",
      "41 1.0\n",
      "42 1.0\n",
      "43 1.0\n",
      "44 1.0\n",
      "45 1.0\n",
      "46 1.0\n",
      "47 1.0\n",
      "48 1.0\n",
      "49 1.0\n",
      "50 1.0\n",
      "51 1.0\n",
      "52 1.0\n",
      "53 1.0\n",
      "54 1.0\n",
      "55 1.0\n",
      "56 1.0\n",
      "57 1.0\n",
      "58 1.0\n",
      "59 1.0\n",
      "60 1.0\n",
      "61 1.0\n",
      "62 1.0\n",
      "63 1.0\n",
      "64 1.0\n",
      "65 1.0\n",
      "66 1.0\n",
      "67 1.0\n",
      "68 1.0\n",
      "69 1.0\n",
      "70 1.0\n",
      "71 1.0\n",
      "72 1.0\n",
      "73 1.0\n",
      "74 1.0\n",
      "75 1.0\n",
      "76 1.0\n",
      "77 1.0\n",
      "78 1.0\n",
      "79 1.0\n",
      "80 1.0\n",
      "81 1.0\n",
      "82 1.0\n",
      "83 1.0\n",
      "84 1.0\n",
      "85 1.0\n",
      "86 1.0\n",
      "87 1.0\n",
      "88 1.0\n",
      "89 1.0\n",
      "90 1.0\n",
      "91 1.0\n",
      "92 1.0\n",
      "93 1.0\n",
      "94 1.0\n",
      "95 1.0\n",
      "96 1.0\n",
      "97 1.0\n",
      "98 1.0\n",
      "99 1.0\n",
      "100 1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# tf Graph Input\n",
    "X = [1, 2, 3]\n",
    "Y = [1, 2, 3]\n",
    "\n",
    "# Set wrong model weights\n",
    "W = tf.Variable(5.0)\n",
    "\n",
    "# Linear model\n",
    "hypothesis = X * W\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize: Gradient Descent Optimizer\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "with tf.Session() as sess:\n",
    "    # Initializes global variables in the graph.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(101):\n",
    "        _, W_val = sess.run([train, W])\n",
    "        print(step, W_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 시간이 지날수록 W가 1로 수렴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 37.333332 [(37.333336, 4.6266665)]\n",
      "1 33.84889 [(33.84889, 4.2881775)]\n",
      "2 30.689657 [(30.689657, 3.9812808)]\n",
      "3 27.825287 [(27.825287, 3.703028)]\n",
      "4 25.228262 [(25.228262, 3.4507453)]\n",
      "5 22.873621 [(22.873623, 3.2220092)]\n",
      "6 20.738752 [(20.73875, 3.0146217)]\n",
      "7 18.803137 [(18.803137, 2.8265903)]\n",
      "8 17.048176 [(17.048176, 2.6561086)]\n",
      "9 15.457013 [(15.457014, 2.5015385)]\n",
      "10 14.014359 [(14.01436, 2.361395)]\n",
      "11 12.706352 [(12.706352, 2.2343314)]\n",
      "12 11.520427 [(11.520427, 2.119127)]\n",
      "13 10.445186 [(10.445185, 2.0146751)]\n",
      "14 9.470302 [(9.470302, 1.9199722)]\n",
      "15 8.586407 [(8.586407, 1.8341081)]\n",
      "16 7.785009 [(7.785009, 1.756258)]\n",
      "17 7.0584083 [(7.0584083, 1.685674)]\n",
      "18 6.399624 [(6.399624, 1.6216778)]\n",
      "19 5.8023257 [(5.8023252, 1.5636545)]\n",
      "20 5.260776 [(5.260776, 1.5110468)]\n",
      "21 4.7697697 [(4.7697697, 1.4633491)]\n",
      "22 4.324591 [(4.324591, 1.4201032)]\n",
      "23 3.9209633 [(3.9209635, 1.3808936)]\n",
      "24 3.5550067 [(3.5550067, 1.3453435)]\n",
      "25 3.2232056 [(3.2232056, 1.3131114)]\n",
      "26 2.9223735 [(2.9223735, 1.2838877)]\n",
      "27 2.6496189 [(2.6496186, 1.2573916)]\n",
      "28 2.4023216 [(2.4023216, 1.2333684)]\n",
      "29 2.178105 [(2.178105, 1.2115873)]\n",
      "30 1.9748148 [(1.9748147, 1.1918392)]\n",
      "31 1.7904993 [(1.7904994, 1.1739342)]\n",
      "32 1.623386 [(1.6233861, 1.1577003)]\n",
      "33 1.4718695 [(1.4718695, 1.1429816)]\n",
      "34 1.3344955 [(1.3344957, 1.1296366)]\n",
      "35 1.2099417 [(1.2099419, 1.1175373)]\n",
      "36 1.0970144 [(1.0970144, 1.1065671)]\n",
      "37 0.9946267 [(0.9946267, 1.0966209)]\n",
      "38 0.90179497 [(0.901795, 1.087603)]\n",
      "39 0.8176275 [(0.81762755, 1.0794266)]\n",
      "40 0.7413151 [(0.7413151, 1.0720135)]\n",
      "41 0.67212623 [(0.67212623, 1.0652922)]\n",
      "42 0.609394 [(0.609394, 1.0591983)]\n",
      "43 0.5525169 [(0.5525169, 1.0536731)]\n",
      "44 0.50094914 [(0.50094914, 1.0486636)]\n",
      "45 0.45419374 [(0.45419377, 1.0441216)]\n",
      "46 0.41180158 [(0.41180158, 1.0400037)]\n",
      "47 0.37336722 [(0.37336725, 1.03627)]\n",
      "48 0.33851996 [(0.33852, 1.0328848)]\n",
      "49 0.30692515 [(0.30692515, 1.0298156)]\n",
      "50 0.27827826 [(0.2782783, 1.0270327)]\n",
      "51 0.25230527 [(0.25230527, 1.0245097)]\n",
      "52 0.2287569 [(0.2287569, 1.022222)]\n",
      "53 0.20740573 [(0.20740573, 1.020148)]\n",
      "54 0.18804836 [(0.18804836, 1.0182675)]\n",
      "55 0.17049654 [(0.17049655, 1.0165626)]\n",
      "56 0.15458433 [(0.15458433, 1.0150168)]\n",
      "57 0.14015675 [(0.14015675, 1.0136153)]\n",
      "58 0.12707591 [(0.12707591, 1.0123445)]\n",
      "59 0.11521538 [(0.11521538, 1.0111923)]\n",
      "60 0.10446167 [(0.10446167, 1.0101477)]\n",
      "61 0.09471202 [(0.09471202, 1.0092006)]\n",
      "62 0.08587202 [(0.08587202, 1.0083419)]\n",
      "63 0.07785805 [(0.07785805, 1.0075634)]\n",
      "64 0.07059129 [(0.07059129, 1.0068574)]\n",
      "65 0.06400236 [(0.06400236, 1.0062174)]\n",
      "66 0.05802846 [(0.05802846, 1.005637)]\n",
      "67 0.052612226 [(0.052612226, 1.005111)]\n",
      "68 0.047702473 [(0.047702473, 1.0046339)]\n",
      "69 0.043249767 [(0.043249767, 1.0042014)]\n",
      "70 0.03921318 [(0.03921318, 1.0038093)]\n",
      "71 0.035553534 [(0.035553537, 1.0034539)]\n",
      "72 0.032236177 [(0.03223618, 1.0031315)]\n",
      "73 0.029227654 [(0.029227655, 1.0028392)]\n",
      "74 0.02649951 [(0.02649951, 1.0025742)]\n",
      "75 0.024025917 [(0.024025917, 1.002334)]\n",
      "76 0.021783749 [(0.02178375, 1.0021162)]\n",
      "77 0.01975123 [(0.019751232, 1.0019187)]\n",
      "78 0.017907381 [(0.017907381, 1.0017396)]\n",
      "79 0.016236702 [(0.016236704, 1.0015773)]\n",
      "80 0.014720838 [(0.014720838, 1.00143)]\n",
      "81 0.01334699 [(0.013346991, 1.0012965)]\n",
      "82 0.012100856 [(0.012100856, 1.0011755)]\n",
      "83 0.010971785 [(0.010971785, 1.0010659)]\n",
      "84 0.0099481745 [(0.0099481745, 1.0009663)]\n",
      "85 0.009018898 [(0.009018898, 1.0008761)]\n",
      "86 0.008176883 [(0.008176884, 1.0007943)]\n",
      "87 0.007413149 [(0.007413149, 1.0007201)]\n",
      "88 0.006721576 [(0.006721576, 1.0006529)]\n",
      "89 0.0060940585 [(0.0060940585, 1.000592)]\n",
      "90 0.005525271 [(0.0055252714, 1.0005368)]\n",
      "91 0.0050098896 [(0.0050098896, 1.0004867)]\n",
      "92 0.004542589 [(0.004542589, 1.0004413)]\n",
      "93 0.0041189194 [(0.0041189194, 1.0004001)]\n",
      "94 0.0037339528 [(0.003733953, 1.0003628)]\n",
      "95 0.0033854644 [(0.0033854644, 1.0003289)]\n",
      "96 0.0030694802 [(0.0030694804, 1.0002983)]\n",
      "97 0.0027837753 [(0.0027837753, 1.0002704)]\n",
      "98 0.0025234222 [(0.0025234222, 1.0002451)]\n",
      "99 0.0022875469 [(0.0022875469, 1.0002222)]\n",
      "100 0.0020739238 [(0.0020739238, 1.0002015)]\n"
     ]
    }
   ],
   "source": [
    "# This is optional\n",
    "import tensorflow as tf\n",
    "\n",
    "# tf Graph Input\n",
    "X = [1, 2, 3]\n",
    "Y = [1, 2, 3]\n",
    "\n",
    "# Set wrong model weights\n",
    "W = tf.Variable(5.)\n",
    "\n",
    "# Linear model\n",
    "hypothesis = X * W\n",
    "\n",
    "# Manual gradient\n",
    "gradient = tf.reduce_mean((W * X - Y) * X) * 2\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize: Gradient Descent Optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "\n",
    "# Get gradients\n",
    "gvs = optimizer.compute_gradients(cost,[W])\n",
    "\n",
    "# Optional: modify gradient if necessary\n",
    "# gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
    "\n",
    "# Apply gradients\n",
    "apply_gradients = optimizer.apply_gradients(gvs)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "with tf.Session() as sess:\n",
    "    # Initializes global variables in the graph.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(101):\n",
    "        gradient_val, gvs_val, _ = sess.run([gradient, gvs, apply_gradients])\n",
    "        print(step, gradient_val, gvs_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
