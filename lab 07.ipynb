{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Learning rate  \n",
    "    - large learning rate : overshooting  \n",
    "    - small learning rate : takes too long, stops at local minimum  \n",
    "2. Data preprocessing  \n",
    "3. Overfitting  \n",
    "    - More training data  \n",
    "    - Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lab 07-1 learning rate and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.8089976 [[-0.19632228 -1.220749    0.1588957 ]\n",
      " [-0.17167972 -0.65690064  0.47665888]\n",
      " [ 0.10662136  1.1824698  -0.09887639]]\n",
      "1 2.3408751 [[-0.17383566 -1.1944515   0.11011147]\n",
      " [-0.01688144 -0.5149655   0.17992541]\n",
      " [ 0.26038107  1.3139198  -0.38408604]]\n",
      "2 1.0000141 [[-0.16864993 -1.209269    0.11974332]\n",
      " [ 0.05451432 -0.5900049   0.1835691 ]\n",
      " [ 0.33994037  1.2201968  -0.36992246]]\n",
      "3 0.92962295 [[-1.8655542e-01 -1.2006503e+00  1.2903014e-01]\n",
      " [ 1.1226237e-03 -5.3850079e-01  1.8545668e-01]\n",
      " [ 2.9587930e-01  1.2550519e+00 -3.6071643e-01]]\n",
      "4 0.9123956 [[-0.18835762 -1.2053543   0.1355364 ]\n",
      " [ 0.03421821 -0.5597148   0.1735751 ]\n",
      " [ 0.33680105  1.2151657  -0.3617521 ]]\n",
      "5 0.8993304 [[-0.20125508 -1.200564    0.1436435 ]\n",
      " [ 0.00744396 -0.52958065  0.1702152 ]\n",
      " [ 0.31867573  1.2283405  -0.35680148]]\n",
      "6 0.89160913 [[-0.20631352 -1.2024702   0.15060808]\n",
      " [ 0.0228715  -0.5362047   0.16141172]\n",
      " [ 0.3419734   1.2039992  -0.3557578 ]]\n",
      "7 0.8850377 [[-0.21703993 -1.1994644   0.1583288 ]\n",
      " [ 0.00766051 -0.5164216   0.15683962]\n",
      " [ 0.33485726  1.2071297  -0.35177225]]\n",
      "8 0.87953126 [[-0.22358076 -1.2000313   0.16543648]\n",
      " [ 0.01496029 -0.51640975  0.14952798]\n",
      " [ 0.3497369   1.190262   -0.34978414]]\n",
      "9 0.8744389 [[-0.23317349 -1.197895    0.17289293]\n",
      " [ 0.00572609 -0.5020176   0.14437005]\n",
      " [ 0.3480625   1.1885064  -0.34635416]]\n",
      "10 0.86968327 [[-0.2404392  -1.1977352   0.17999873]\n",
      " [ 0.00897892 -0.49870524  0.13780485]\n",
      " [ 0.3585015   1.1756971  -0.34398386]]\n",
      "11 0.8651273 [[-0.24939789 -1.1960362   0.18725847]\n",
      " [ 0.00302666 -0.4873436   0.13239548]\n",
      " [ 0.35961282  1.1714882  -0.3408863 ]]\n",
      "12 0.8607378 [[-0.25702178 -1.1954569   0.19430304]\n",
      " [ 0.00420844 -0.4823917   0.12626177]\n",
      " [ 0.3675618   1.1610236  -0.33837077]]\n",
      "13 0.85647887 [[-2.6560429e-01 -1.1939751e+00  2.0140383e-01]\n",
      " [ 1.4527841e-04 -4.7284645e-01  1.2077971e-01]\n",
      " [ 3.7010059e-01  1.1556010e+00 -3.3548701e-01]]\n",
      "14 0.852337 [[-2.73397654e-01 -1.19313896e+00  2.08361119e-01]\n",
      " [ 2.70689547e-04 -4.67122912e-01  1.14930764e-01]\n",
      " [ 3.76576215e-01  1.14657629e+00 -3.32937926e-01]]\n",
      "15 0.84830034 [[-0.2817416  -1.1917582   0.21532431]\n",
      " [-0.00264947 -0.45874232  0.10947036]\n",
      " [ 0.37982577  1.1405962  -0.33020732]]\n",
      "16 0.8443622 [[-0.28960392 -1.1907545   0.222183  ]\n",
      " [-0.00304289 -0.45271257  0.10383404]\n",
      " [ 0.38537866  1.132517   -0.3276811 ]]\n",
      "17 0.84051746 [[-0.2977847  -1.1894121   0.22902131]\n",
      " [-0.00523194 -0.44513482  0.09844534]\n",
      " [ 0.3889532   1.1263347  -0.32507324]]\n",
      "18 0.83676195 [[-0.30566108 -1.1882914   0.23577705]\n",
      " [-0.00585383 -0.43905237  0.09298479]\n",
      " [ 0.39389166  1.118923   -0.3226    ]]\n",
      "19 0.833092 [[-0.31372094 -1.1869532   0.24249862]\n",
      " [-0.00754618 -0.4320699   0.08769467]\n",
      " [ 0.39758113  1.1127334  -0.32009992]]\n",
      "20 0.8295046 [[-0.32158077 -1.1857451   0.24915044]\n",
      " [-0.00823812 -0.42607373  0.08239045]\n",
      " [ 0.40208364  1.1058276  -0.31769657]]\n",
      "21 0.82599705 [[-0.32954395 -1.1843925   0.25576103]\n",
      " [-0.0095718  -0.419563    0.07721341]\n",
      " [ 0.40577328  1.0997385  -0.31529713]]\n",
      "22 0.8225665 [[-0.33736977 -1.1831152   0.2623097 ]\n",
      " [-0.01024745 -0.4137288   0.07205489]\n",
      " [ 0.40994716  1.093241   -0.3129735 ]]\n",
      "23 0.81921047 [[-0.34525052 -1.1817385   0.26881367]\n",
      " [-0.01130714 -0.40761292  0.06699868]\n",
      " [ 0.4135748   1.0873113  -0.31067148]]\n",
      "24 0.8159264 [[-0.3530323  -1.1804038   0.2752608 ]\n",
      " [-0.01192026 -0.40198073  0.06197963]\n",
      " [ 0.41748753  1.0811607  -0.3084336 ]]\n",
      "25 0.8127122 [[-0.3608391  -1.178998    0.28166178]\n",
      " [-0.01276031 -0.39620936  0.0570483 ]\n",
      " [ 0.42102078  1.0754215  -0.30622762]]\n",
      "26 0.8095652 [[-0.3685709  -1.1776139   0.28800946]\n",
      " [-0.01328746 -0.39079875  0.05216485]\n",
      " [ 0.42471683  1.069577   -0.30407917]]\n",
      "27 0.80648345 [[-0.37630904 -1.1761769   0.29431066]\n",
      " [-0.01394465 -0.38533694  0.04736023]\n",
      " [ 0.42814013  1.0640433  -0.30196875]]\n",
      "28 0.8034649 [[-0.38398737 -1.1747491   0.3005612 ]\n",
      " [-0.01437546 -0.38015553  0.04260965]\n",
      " [ 0.43165052  1.0584761  -0.29991195]]\n",
      "29 0.8005074 [[-0.39166015 -1.1732807   0.30676553]\n",
      " [-0.01487617 -0.3749774   0.03793225]\n",
      " [ 0.43495807  1.0531534  -0.29789683]]\n",
      "30 0.79760873 [[-0.399283   -1.1718136   0.3129213 ]\n",
      " [-0.0152079  -0.37002572  0.0333123 ]\n",
      " [ 0.43830553  1.0478421  -0.295933  ]]\n",
      "31 0.79476744 [[-0.40689257 -1.1703142   0.3190315 ]\n",
      " [-0.01557196 -0.36511117  0.02876182]\n",
      " [ 0.4414973   1.0427303  -0.29401293]]\n",
      "32 0.79198134 [[-0.41445866 -1.1688114   0.32509488]\n",
      " [-0.01580623 -0.36038563  0.02427056]\n",
      " [ 0.44469953  1.0376577  -0.2921426 ]]\n",
      "33 0.78924876 [[-0.42200652 -1.1672823   0.33111358]\n",
      " [-0.01604963 -0.35571775  0.01984607]\n",
      " [ 0.4477785   1.0327533  -0.2903172 ]]\n",
      "34 0.786568 [[-0.4295152  -1.165747    0.33708698]\n",
      " [-0.01619059 -0.35121226  0.01548155]\n",
      " [ 0.45084983  1.0279052  -0.28854042]]\n",
      "35 0.7839373 [[-0.43700233 -1.1641896   0.34301665]\n",
      " [-0.01632646 -0.34677657  0.01118175]\n",
      " [ 0.45382097  1.0232028  -0.28680912]]\n",
      "36 0.78135514 [[-0.44445327 -1.1626244   0.34890243]\n",
      " [-0.01637962 -0.3424837   0.00694204]\n",
      " [ 0.45677355  1.0185666  -0.2851255 ]]\n",
      "37 0.7788201 [[-0.45188043 -1.1610403   0.35474554]\n",
      " [-0.01641933 -0.3382673   0.00276534]\n",
      " [ 0.45964274  1.0140593  -0.2834874 ]]\n",
      "38 0.7763305 [[-0.4592735  -1.1594477   0.36054602]\n",
      " [-0.01639101 -0.33417866 -0.00135161]\n",
      " [ 0.46248695  1.0096239  -0.28189617]]\n",
      "39 0.7738849 [[-0.4666413  -1.1578387   0.36630487]\n",
      " [-0.01634453 -0.3301697  -0.00540706]\n",
      " [ 0.4652605   1.0053045  -0.28035027]]\n",
      "40 0.7714821 [[-0.4739765  -1.1562209   0.37202233]\n",
      " [-0.0162412  -0.3262767  -0.00940339]\n",
      " [ 0.46800572  1.0010594  -0.2788504 ]]\n",
      "41 0.76912045 [[-0.4812855  -1.1545888   0.37769923]\n",
      " [-0.01611752 -0.32246414 -0.01333962]\n",
      " [ 0.47068998  0.99692017 -0.27739543]]\n",
      "42 0.7667992 [[-0.488563   -1.152948    0.3833359 ]\n",
      " [-0.01594573 -0.318758   -0.01721754]\n",
      " [ 0.47334453  0.9928558  -0.27598563]]\n",
      "43 0.7645166 [[-0.49581367 -1.1512946   0.38893306]\n",
      " [-0.01575297 -0.3151316  -0.02103671]\n",
      " [ 0.47594583  0.9888889  -0.2746201 ]]\n",
      "44 0.76227176 [[-0.50303364 -1.1496327   0.39449114]\n",
      " [-0.01551916 -0.31160355 -0.02479856]\n",
      " [ 0.47851726  0.98499626 -0.27329886]]\n",
      "45 0.7600635 [[-0.5102264  -1.1479596   0.40001076]\n",
      " [-0.01526468 -0.30815363 -0.02850297]\n",
      " [ 0.4810418   0.981194   -0.2720211 ]]\n",
      "46 0.7578907 [[-0.5173892  -1.1462785   0.40549237]\n",
      " [-0.014975   -0.30479515 -0.03215114]\n",
      " [ 0.48353702  0.97746444 -0.27078673]]\n",
      "47 0.7557522 [[-0.5245245  -1.1445873   0.41093653]\n",
      " [-0.01466561 -0.30151248 -0.0357432 ]\n",
      " [ 0.48599055  0.9738192  -0.26959503]]\n",
      "48 0.7536472 [[-0.53163034 -1.1428887   0.41634372]\n",
      " [-0.0143259  -0.29831523 -0.03928015]\n",
      " [ 0.48841593  0.97024447 -0.26844567]]\n",
      "49 0.7515746 [[-0.5387087  -1.1411811   0.4217145 ]\n",
      " [-0.01396787 -0.2951911  -0.04276232]\n",
      " [ 0.490804    0.9667488  -0.26733804]]\n",
      "50 0.7495334 [[-0.545758   -1.1394666   0.42704934]\n",
      " [-0.01358367 -0.292147   -0.04619062]\n",
      " [ 0.49316528  0.96332115 -0.26627174]]\n",
      "51 0.7475227 [[-0.55277985 -1.1377442   0.43234873]\n",
      " [-0.0131827  -0.28917316 -0.04956545]\n",
      " [ 0.4954931   0.95996773 -0.26524615]]\n",
      "52 0.74554193 [[-0.5597731  -1.1360154   0.43761313]\n",
      " [-0.01275917 -0.28627446 -0.05288768]\n",
      " [ 0.49779573  0.9566798  -0.26426086]]\n",
      "53 0.7435898 [[-0.5667389  -1.1342795   0.44284308]\n",
      " [-0.01232054 -0.283443   -0.05615778]\n",
      " [ 0.5000681   0.95346177 -0.26331523]]\n",
      "54 0.74166584 [[-0.57367647 -1.1325378   0.448039  ]\n",
      " [-0.01186248 -0.2806823  -0.05937653]\n",
      " [ 0.502317    0.9503065  -0.26240876]]\n",
      "55 0.7397691 [[-0.5805868  -1.1307899   0.45320138]\n",
      " [-0.01139105 -0.27798572 -0.0625445 ]\n",
      " [ 0.50453854  0.9472171  -0.26154086]]\n",
      "56 0.7378988 [[-0.58746916 -1.1290368   0.4583307 ]\n",
      " [-0.01090296 -0.27535588 -0.06566245]\n",
      " [ 0.5067381   0.94418764 -0.260711  ]]\n",
      "57 0.7360543 [[-0.59432447 -1.1272782   0.46342736]\n",
      " [-0.01040319 -0.27278712 -0.06873097]\n",
      " [ 0.5089129   0.94122046 -0.25991863]]\n",
      "58 0.7342349 [[-0.6011522  -1.125515    0.46849188]\n",
      " [-0.00988919 -0.2702813  -0.07175079]\n",
      " [ 0.51106745  0.93831044 -0.25916314]]\n",
      "59 0.73244005 [[-0.6079531  -1.1237469   0.47352466]\n",
      " [-0.00936516 -0.2678336  -0.07472252]\n",
      " [ 0.5131994   0.9354594  -0.25844398]]\n",
      "60 0.73066884 [[-0.61472666 -1.1219747   0.47852612]\n",
      " [-0.008829   -0.26544538 -0.07764688]\n",
      " [ 0.5153127   0.93266267 -0.25776058]]\n",
      "61 0.7289209 [[-0.62147367 -1.1201984   0.48349673]\n",
      " [-0.0082844  -0.26311237 -0.0805245 ]\n",
      " [ 0.5174054   0.92992175 -0.25711235]]\n",
      "62 0.7271955 [[-0.62819374 -1.1184185   0.48843688]\n",
      " [-0.0077297  -0.2608355  -0.08335607]\n",
      " [ 0.5194809   0.9272326  -0.25649872]]\n",
      "63 0.72549194 [[-0.6348874  -1.116635    0.493347  ]\n",
      " [-0.00716791 -0.2586111  -0.08614223]\n",
      " [ 0.5215377   0.9245962  -0.25591913]]\n",
      "64 0.7238099 [[-0.6415545  -1.1148484   0.49822748]\n",
      " [-0.00659786 -0.25643972 -0.08888367]\n",
      " [ 0.5235786   0.92200917 -0.25537297]]\n",
      "65 0.7221488 [[-0.64819545 -1.1130587   0.50307876]\n",
      " [-0.00602197 -0.25431827 -0.09158102]\n",
      " [ 0.5256026   0.91947186 -0.2548597 ]]\n",
      "66 0.72050786 [[-0.65481013 -1.1112665   0.50790125]\n",
      " [-0.00543949 -0.2522468  -0.09423498]\n",
      " [ 0.52761185  0.91698164 -0.2543787 ]]\n",
      "67 0.71888685 [[-0.661399   -1.1094717   0.5126953 ]\n",
      " [-0.00485239 -0.2502227  -0.09684616]\n",
      " [ 0.52960575  0.91453844 -0.25392944]]\n",
      "68 0.7172852 [[-0.66796196 -1.1076747   0.5174613 ]\n",
      " [-0.0042601  -0.24824594 -0.09941524]\n",
      " [ 0.5315861   0.91213995 -0.2535113 ]]\n",
      "69 0.71570253 [[-0.67449933 -1.1058757   0.5221997 ]\n",
      " [-0.00366433 -0.2463141  -0.10194284]\n",
      " [ 0.53355247  0.90978605 -0.2531238 ]]\n",
      "70 0.71413815 [[-0.68101114 -1.1040751   0.5269108 ]\n",
      " [-0.0030647  -0.24442695 -0.10442962]\n",
      " [ 0.53550637  0.90747464 -0.25276628]]\n",
      "71 0.7125919 [[-0.6874977  -1.1022727   0.531595  ]\n",
      " [-0.00246258 -0.24258247 -0.10687622]\n",
      " [ 0.5374476   0.90520537 -0.25243825]]\n",
      "72 0.7110632 [[-0.693959   -1.1004691   0.5362526 ]\n",
      " [-0.00185781 -0.24078019 -0.10928328]\n",
      " [ 0.5393773   0.9029765  -0.25213915]]\n",
      "73 0.70955163 [[-0.70039535 -1.0986643   0.5408841 ]\n",
      " [-0.00125147 -0.23901841 -0.11165141]\n",
      " [ 0.5412954   0.9007876  -0.2518684 ]]\n",
      "74 0.7080569 [[-7.06806779e-01 -1.09685850e+00  5.45489728e-01]\n",
      " [-6.43488078e-04 -2.37296566e-01 -1.13981225e-01]\n",
      " [ 5.43203056e-01  8.98637056e-01 -2.51625478e-01]]\n",
      "75 0.70657843 [[-7.1319360e-01 -1.0950519e+00  5.5006987e-01]\n",
      " [-3.4836703e-05 -2.3561309e-01 -1.1627336e-01]\n",
      " [ 5.4510009e-01  8.9652437e-01 -2.5140983e-01]]\n",
      "76 0.70511615 [[-7.1955585e-01 -1.0932447e+00  5.5462486e-01]\n",
      " [ 5.7456119e-04 -2.3396742e-01 -1.1852840e-01]\n",
      " [ 5.4698753e-01  8.9444804e-01 -2.5122091e-01]]\n",
      "77 0.7036694 [[-0.7258938  -1.091437    0.55915505]\n",
      " [ 0.00118378 -0.23235807 -0.12074698]\n",
      " [ 0.54886526  0.89240766 -0.25105822]]\n",
      "78 0.7022381 [[-0.7322075  -1.089629    0.5636608 ]\n",
      " [ 0.001793   -0.23078457 -0.12292968]\n",
      " [ 0.5507342   0.8904017  -0.25092125]]\n",
      "79 0.7008216 [[-0.7384972  -1.0878209   0.5681424 ]\n",
      " [ 0.00240123 -0.2292454  -0.12507708]\n",
      " [ 0.5525942   0.88842994 -0.25080946]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 0.6994198 [[-0.744763   -1.0860128   0.5726002 ]\n",
      " [ 0.00300882 -0.22774027 -0.1271898 ]\n",
      " [ 0.5544462   0.88649076 -0.25072235]]\n",
      "81 0.6980323 [[-0.7510052  -1.0842049   0.5770345 ]\n",
      " [ 0.00361476 -0.22626762 -0.1292684 ]\n",
      " [ 0.55629003  0.884584   -0.25065944]]\n",
      "82 0.69665897 [[-0.7572238  -1.0823975   0.5814456 ]\n",
      " [ 0.00421935 -0.22482717 -0.13131341]\n",
      " [ 0.5581265   0.8827083  -0.2506202 ]]\n",
      "83 0.6952992 [[-0.7634191  -1.0805904   0.5858338 ]\n",
      " [ 0.0048218  -0.22341758 -0.13332546]\n",
      " [ 0.55995554  0.88086325 -0.25060415]]\n",
      "84 0.69395304 [[-0.76959115 -1.078784    0.5901994 ]\n",
      " [ 0.00542223 -0.2220384  -0.13530506]\n",
      " [ 0.5617778   0.8790477  -0.25061083]]\n",
      "85 0.69262 [[-0.77574027 -1.0769782   0.5945428 ]\n",
      " [ 0.00602004 -0.22068849 -0.1372528 ]\n",
      " [ 0.56359315  0.8772612  -0.25063977]]\n",
      "86 0.6912998 [[-0.7818665  -1.0751734   0.5988642 ]\n",
      " [ 0.00661539 -0.21936746 -0.13916917]\n",
      " [ 0.5654024   0.8755027  -0.25069046]]\n",
      "87 0.6899923 [[-0.78797007 -1.0733695   0.6031639 ]\n",
      " [ 0.00720761 -0.2180741  -0.14105475]\n",
      " [ 0.56720525  0.87377185 -0.25076246]]\n",
      "88 0.68869716 [[-0.7940511  -1.0715667   0.60744214]\n",
      " [ 0.00779688 -0.21680807 -0.14291005]\n",
      " [ 0.56900245  0.8720676  -0.25085536]]\n",
      "89 0.68741417 [[-0.8001098  -1.0697651   0.6116993 ]\n",
      " [ 0.00838271 -0.21556836 -0.14473559]\n",
      " [ 0.57079387  0.87038946 -0.25096866]]\n",
      "90 0.68614304 [[-0.8061463  -1.0679649   0.61593556]\n",
      " [ 0.00896513 -0.21435447 -0.14653188]\n",
      " [ 0.57258     0.8687366  -0.25110194]]\n",
      "91 0.6848836 [[-0.81216085 -1.066166    0.6201512 ]\n",
      " [ 0.00954378 -0.21316557 -0.14829944]\n",
      " [ 0.57436085  0.8671086  -0.25125477]]\n",
      "92 0.68363565 [[-0.8181535  -1.0643687   0.62434655]\n",
      " [ 0.01011867 -0.21200112 -0.15003878]\n",
      " [ 0.5761368   0.86550456 -0.25142676]]\n",
      "93 0.68239886 [[-0.82412446 -1.0625731   0.62852186]\n",
      " [ 0.01068943 -0.21086031 -0.15175036]\n",
      " [ 0.5779079   0.86392415 -0.25161743]]\n",
      "94 0.68117315 [[-0.8300739  -1.0607791   0.6326774 ]\n",
      " [ 0.01125621 -0.20974278 -0.15343466]\n",
      " [ 0.5796746   0.8623665  -0.2518264 ]]\n",
      "95 0.6799582 [[-0.836002   -1.0589869   0.63681334]\n",
      " [ 0.01181854 -0.20864756 -0.15509218]\n",
      " [ 0.5814367   0.8608313  -0.25205332]]\n",
      "96 0.67875385 [[-0.8419089  -1.0571966   0.64093   ]\n",
      " [ 0.01237666 -0.20757449 -0.15672338]\n",
      " [ 0.58319473  0.8593177  -0.25229773]]\n",
      "97 0.67755985 [[-0.8477948  -1.0554084   0.6450276 ]\n",
      " [ 0.01293009 -0.20652258 -0.15832873]\n",
      " [ 0.5849485   0.85782546 -0.25255927]]\n",
      "98 0.6763762 [[-0.85365975 -1.0536221   0.6491064 ]\n",
      " [ 0.0134791  -0.20549163 -0.15990867]\n",
      " [ 0.5866985   0.8563537  -0.25283754]]\n",
      "99 0.6752025 [[-0.8595041  -1.051838    0.6531666 ]\n",
      " [ 0.0140232  -0.20448075 -0.16146366]\n",
      " [ 0.58844453  0.8549023  -0.2531322 ]]\n",
      "100 0.67403865 [[-0.86532784 -1.0500562   0.6572085 ]\n",
      " [ 0.01456273 -0.20348981 -0.16299413]\n",
      " [ 0.59018713  0.8534704  -0.25344285]]\n",
      "101 0.67288446 [[-0.87113124 -1.0482765   0.6612323 ]\n",
      " [ 0.01509717 -0.20251788 -0.16450052]\n",
      " [ 0.59192604  0.8520578  -0.2537692 ]]\n",
      "102 0.6717398 [[-0.8769144  -1.0464993   0.6652382 ]\n",
      " [ 0.01562687 -0.20156486 -0.16598324]\n",
      " [ 0.5936617   0.8506637  -0.25411078]]\n",
      "103 0.6706045 [[-0.8826775  -1.0447243   0.66922647]\n",
      " [ 0.01615139 -0.20062989 -0.16744274]\n",
      " [ 0.595394    0.84928805 -0.25446734]]\n",
      "104 0.6694785 [[-0.8884207  -1.042952    0.67319727]\n",
      " [ 0.01667101 -0.19971284 -0.1688794 ]\n",
      " [ 0.59712327  0.84792995 -0.2548385 ]]\n",
      "105 0.6683613 [[-0.8941442  -1.041182    0.67715085]\n",
      " [ 0.01718533 -0.19881293 -0.17029364]\n",
      " [ 0.5988493   0.8465894  -0.25522396]]\n",
      "106 0.6672531 [[-0.89984804 -1.0394148   0.68108743]\n",
      " [ 0.01769465 -0.19793004 -0.17168586]\n",
      " [ 0.6005725   0.84526557 -0.25562337]]\n",
      "107 0.6661536 [[-0.9055325  -1.0376501   0.6850072 ]\n",
      " [ 0.01819864 -0.19706345 -0.17305642]\n",
      " [ 0.6022928   0.8439583  -0.25603643]]\n",
      "108 0.6650626 [[-0.91119766 -1.0358882   0.6889104 ]\n",
      " [ 0.01869743 -0.19621293 -0.17440575]\n",
      " [ 0.60401034  0.8426671  -0.2564628 ]]\n",
      "109 0.66398025 [[-0.9168437  -1.034129    0.69279724]\n",
      " [ 0.01919091 -0.19537795 -0.17573422]\n",
      " [ 0.60572517  0.8413917  -0.25690222]]\n",
      "110 0.662906 [[-0.92247075 -1.0323726   0.69666785]\n",
      " [ 0.01967914 -0.19455822 -0.17704217]\n",
      " [ 0.60743743  0.8401315  -0.25735435]]\n",
      "111 0.66183984 [[-0.92807895 -1.030619    0.7005225 ]\n",
      " [ 0.02016198 -0.19375324 -0.17832999]\n",
      " [ 0.60914713  0.8388864  -0.2578189 ]]\n",
      "112 0.66078186 [[-0.9336685  -1.0288683   0.7043613 ]\n",
      " [ 0.02063953 -0.19296277 -0.17959802]\n",
      " [ 0.6108544   0.8376558  -0.2582956 ]]\n",
      "113 0.6597318 [[-0.9392395  -1.0271205   0.70818454]\n",
      " [ 0.0211117  -0.19218634 -0.18084662]\n",
      " [ 0.61255926  0.8364395  -0.25878415]]\n",
      "114 0.6586895 [[-0.94479215 -1.0253756   0.7119923 ]\n",
      " [ 0.02157853 -0.19142365 -0.18207613]\n",
      " [ 0.6142618   0.8352371  -0.25928426]]\n",
      "115 0.65765476 [[-0.95032656 -1.0236338   0.7157849 ]\n",
      " [ 0.02203996 -0.19067433 -0.18328689]\n",
      " [ 0.615962    0.8340483  -0.2597957 ]]\n",
      "116 0.65662766 [[-0.95584285 -1.021895    0.7195624 ]\n",
      " [ 0.02249607 -0.18993808 -0.18447925]\n",
      " [ 0.6176601   0.8328727  -0.2603182 ]]\n",
      "117 0.65560794 [[-0.9613412  -1.0201594   0.7233251 ]\n",
      " [ 0.02294675 -0.18921448 -0.18565354]\n",
      " [ 0.619356    0.8317101  -0.26085147]]\n",
      "118 0.6545956 [[-0.96682173 -1.0184268   0.727073  ]\n",
      " [ 0.02339212 -0.18850334 -0.18681006]\n",
      " [ 0.62104976  0.8305601  -0.2613953 ]]\n",
      "119 0.6535903 [[-0.9722846  -1.0166973   0.7308064 ]\n",
      " [ 0.02383205 -0.18780418 -0.18794914]\n",
      " [ 0.6227414   0.8294226  -0.26194942]]\n",
      "120 0.6525923 [[-0.97773    -1.014971    0.7345255 ]\n",
      " [ 0.02426679 -0.18711698 -0.18907104]\n",
      " [ 0.6244312   0.82829696 -0.26251358]]\n",
      "121 0.65160114 [[-0.983158   -1.0132478   0.73823035]\n",
      " [ 0.02469598 -0.18644106 -0.19017614]\n",
      " [ 0.6261188   0.82718337 -0.26308754]]\n",
      "122 0.6506169 [[-0.9885687  -1.011528    0.7419212 ]\n",
      " [ 0.02512009 -0.18577662 -0.1912647 ]\n",
      " [ 0.6278047   0.82608104 -0.2636711 ]]\n",
      "123 0.6496393 [[-0.9939624  -1.0098114   0.7455982 ]\n",
      " [ 0.02553863 -0.18512285 -0.192337  ]\n",
      " [ 0.6294884   0.8249902  -0.264264  ]]\n",
      "124 0.6486685 [[-0.99933904 -1.008098    0.7492615 ]\n",
      " [ 0.02595216 -0.18448003 -0.19339335]\n",
      " [ 0.6311705   0.8239101  -0.26486602]]\n",
      "125 0.64770424 [[-1.0046989  -1.006388    0.75291127]\n",
      " [ 0.02636014 -0.18384732 -0.19443403]\n",
      " [ 0.6328505   0.82284105 -0.26547697]]\n",
      "126 0.6467466 [[-1.010042   -1.0046812   0.75654763]\n",
      " [ 0.02676317 -0.18322507 -0.19545932]\n",
      " [ 0.634529    0.82178223 -0.26609662]]\n",
      "127 0.6457952 [[-1.0153686  -1.0029777   0.76017076]\n",
      " [ 0.02716072 -0.18261248 -0.19646947]\n",
      " [ 0.63620543  0.8207339  -0.26672477]]\n",
      "128 0.64485013 [[-1.0206786  -1.0012777   0.76378083]\n",
      " [ 0.02755327 -0.18200976 -0.19746476]\n",
      " [ 0.6378803   0.8196955  -0.2673612 ]]\n",
      "129 0.64391136 [[-1.0259725  -0.999581    0.767378  ]\n",
      " [ 0.02794053 -0.18141632 -0.19844545]\n",
      " [ 0.6395533   0.818667   -0.2680057 ]]\n",
      "130 0.64297867 [[-1.0312501  -0.9978877   0.7709623 ]\n",
      " [ 0.02832278 -0.18083222 -0.19941181]\n",
      " [ 0.64122474  0.817648   -0.2686581 ]]\n",
      "131 0.64205205 [[-1.0365118  -0.99619776  0.774534  ]\n",
      " [ 0.02869982 -0.180257   -0.20036408]\n",
      " [ 0.6428943   0.8166385  -0.26931822]]\n",
      "132 0.6411314 [[-1.0417575  -0.9945113   0.77809316]\n",
      " [ 0.02907194 -0.17969067 -0.2013025 ]\n",
      " [ 0.64456236  0.8156381  -0.26998585]]\n",
      "133 0.64021647 [[-1.0469873  -0.99282825  0.78164   ]\n",
      " [ 0.02943889 -0.1791328  -0.20222732]\n",
      " [ 0.6462286   0.81464684 -0.27066085]]\n",
      "134 0.63930756 [[-1.0522015  -0.99114865  0.7851746 ]\n",
      " [ 0.02980095 -0.17858341 -0.20313877]\n",
      " [ 0.64789337  0.81366426 -0.271343  ]]\n",
      "135 0.63840425 [[-1.0574002  -0.9894725   0.7886971 ]\n",
      " [ 0.03015795 -0.1780421  -0.20403709]\n",
      " [ 0.6495564   0.8126904  -0.2720321 ]]\n",
      "136 0.63750666 [[-1.0625834  -0.9877998   0.79220766]\n",
      " [ 0.03051011 -0.17750885 -0.2049225 ]\n",
      " [ 0.6512179   0.81172484 -0.27272803]]\n",
      "137 0.63661456 [[-1.0677514  -0.9861306   0.7957064 ]\n",
      " [ 0.03085734 -0.17698333 -0.20579524]\n",
      " [ 0.6528777   0.8107676  -0.27343062]]\n",
      "138 0.635728 [[-1.0729041  -0.9844649   0.7991934 ]\n",
      " [ 0.03119977 -0.17646547 -0.20665552]\n",
      " [ 0.65453595  0.8098184  -0.27413967]]\n",
      "139 0.63484687 [[-1.0780418  -0.9828026   0.8026688 ]\n",
      " [ 0.03153734 -0.17595498 -0.20750356]\n",
      " [ 0.6561926   0.80887717 -0.27485508]]\n",
      "140 0.6339711 [[-1.0831645  -0.9811439   0.8061328 ]\n",
      " [ 0.03187019 -0.17545184 -0.20833956]\n",
      " [ 0.65784776  0.8079436  -0.27557665]]\n",
      "141 0.6331006 [[-1.0882723  -0.9794887   0.80958545]\n",
      " [ 0.03219822 -0.17495568 -0.20916374]\n",
      " [ 0.65950125  0.8070177  -0.27630424]]\n",
      "142 0.6322353 [[-1.0933656  -0.97783697  0.8130269 ]\n",
      " [ 0.03252167 -0.17446657 -0.2099763 ]\n",
      " [ 0.66115326  0.8060991  -0.2770377 ]]\n",
      "143 0.6313752 [[-1.0984441  -0.9761888   0.81645733]\n",
      " [ 0.0328404  -0.17398416 -0.21077745]\n",
      " [ 0.66280365  0.8051879  -0.2777769 ]]\n",
      "144 0.6305202 [[-1.1035082  -0.9745441   0.8198768 ]\n",
      " [ 0.03315459 -0.1735084  -0.21156739]\n",
      " [ 0.66445255  0.80428374 -0.2785217 ]]\n",
      "145 0.6296702 [[-1.1085579  -0.972903    0.8232854 ]\n",
      " [ 0.0334642  -0.17303912 -0.21234629]\n",
      " [ 0.6660999   0.8033866  -0.2792719 ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146 0.6288252 [[-1.1135933  -0.97126544  0.8266833 ]\n",
      " [ 0.03376928 -0.17257613 -0.21311435]\n",
      " [ 0.6677457   0.8024964  -0.28002745]]\n",
      "147 0.62798494 [[-1.1186147  -0.9696314   0.8300705 ]\n",
      " [ 0.03406989 -0.17211933 -0.21387175]\n",
      " [ 0.66938996  0.8016128  -0.28078815]]\n",
      "148 0.6271497 [[-1.123622   -0.9680009   0.8334472 ]\n",
      " [ 0.03436612 -0.17166862 -0.21461868]\n",
      " [ 0.6710327   0.8007358  -0.28155392]]\n",
      "149 0.6263193 [[-1.1286153  -0.9663739   0.83681357]\n",
      " [ 0.03465792 -0.17122377 -0.21535532]\n",
      " [ 0.67267394  0.79986525 -0.2823246 ]]\n",
      "150 0.6254934 [[-1.1335948  -0.9647505   0.8401696 ]\n",
      " [ 0.03494541 -0.17078473 -0.21608184]\n",
      " [ 0.67431366  0.79900104 -0.2831001 ]]\n",
      "151 0.6246723 [[-1.1385605  -0.96313065  0.84351546]\n",
      " [ 0.03522856 -0.17035131 -0.21679841]\n",
      " [ 0.67595184  0.798143   -0.28388026]]\n",
      "152 0.6238559 [[-1.1435126  -0.9615143   0.8468512 ]\n",
      " [ 0.03550751 -0.16992345 -0.2175052 ]\n",
      " [ 0.6775885   0.7972911  -0.284665  ]]\n",
      "153 0.62304384 [[-1.1484512  -0.9599015   0.85017705]\n",
      " [ 0.03578217 -0.16950093 -0.21820238]\n",
      " [ 0.6792236   0.7964452  -0.28545415]]\n",
      "154 0.6222365 [[-1.1533763  -0.95829225  0.853493  ]\n",
      " [ 0.03605276 -0.1690838  -0.21889012]\n",
      " [ 0.68085724  0.795605   -0.28624767]]\n",
      "155 0.6214335 [[-1.1582882  -0.95668656  0.8567991 ]\n",
      " [ 0.03631913 -0.16867176 -0.21956855]\n",
      " [ 0.6824893   0.7947707  -0.28704542]]\n",
      "156 0.6206349 [[-1.1631869  -0.9550844   0.8600956 ]\n",
      " [ 0.03658153 -0.16826487 -0.22023784]\n",
      " [ 0.6841199   0.793942   -0.28784725]]\n",
      "157 0.61984056 [[-1.1680725  -0.9534857   0.8633825 ]\n",
      " [ 0.03683979 -0.16786285 -0.22089814]\n",
      " [ 0.6857489   0.79311883 -0.2886531 ]]\n",
      "158 0.6190507 [[-1.172945   -0.95189065  0.86665994]\n",
      " [ 0.03709416 -0.16746575 -0.22154962]\n",
      " [ 0.68737644  0.79230106 -0.28946286]]\n",
      "159 0.618265 [[-1.1778046  -0.9502991   0.869928  ]\n",
      " [ 0.0373445  -0.16707331 -0.22219239]\n",
      " [ 0.6890024   0.7914887  -0.29027644]]\n",
      "160 0.61748356 [[-1.1826514  -0.94871104  0.87318677]\n",
      " [ 0.03759104 -0.16668561 -0.22282661]\n",
      " [ 0.6906269   0.7906815  -0.2910937 ]]\n",
      "161 0.6167062 [[-1.1874855  -0.9471265   0.8764363 ]\n",
      " [ 0.0378336  -0.16630235 -0.22345245]\n",
      " [ 0.6922498   0.7898795  -0.29191458]]\n",
      "162 0.61593294 [[-1.1923069  -0.94554555  0.8796767 ]\n",
      " [ 0.03807246 -0.16592366 -0.22407001]\n",
      " [ 0.6938712   0.78908247 -0.29273897]]\n",
      "163 0.6151638 [[-1.1971158  -0.94396806  0.8829081 ]\n",
      " [ 0.03830742 -0.16554917 -0.22467946]\n",
      " [ 0.69549096  0.78829056 -0.2935668 ]]\n",
      "164 0.61439866 [[-1.2019122  -0.94239414  0.8861306 ]\n",
      " [ 0.03853886 -0.16517918 -0.2252809 ]\n",
      " [ 0.69710934  0.7875033  -0.29439792]]\n",
      "165 0.61363745 [[-1.2066963  -0.9408237   0.8893442 ]\n",
      " [ 0.03876637 -0.1648131  -0.22587448]\n",
      " [ 0.69872594  0.7867211  -0.29523233]]\n",
      "166 0.6128802 [[-1.2114681  -0.9392567   0.8925491 ]\n",
      " [ 0.03899051 -0.16445139 -0.22646032]\n",
      " [ 0.7003413   0.7859433  -0.29606986]]\n",
      "167 0.6121268 [[-1.2162278  -0.93769324  0.8957453 ]\n",
      " [ 0.03921079 -0.1640934  -0.22703858]\n",
      " [ 0.7019548   0.7851705  -0.2969105 ]]\n",
      "168 0.61137724 [[-1.2209753  -0.9361333   0.8989329 ]\n",
      " [ 0.03942781 -0.16373967 -0.22760934]\n",
      " [ 0.703567    0.7844019  -0.2977541 ]]\n",
      "169 0.6106316 [[-1.2257109  -0.9345768   0.90211195]\n",
      " [ 0.039641   -0.16338946 -0.22817273]\n",
      " [ 0.7051774   0.78363806 -0.29860067]]\n",
      "170 0.6098896 [[-1.2304345  -0.9330238   0.9052826 ]\n",
      " [ 0.039851   -0.16304329 -0.22872889]\n",
      " [ 0.7067865   0.7828784  -0.29945004]]\n",
      "171 0.6091515 [[-1.2351464  -0.93147427  0.90844494]\n",
      " [ 0.04005736 -0.16270061 -0.22927792]\n",
      " [ 0.7083938   0.78212327 -0.30030218]]\n",
      "172 0.6084168 [[-1.2398465  -0.92992824  0.911599  ]\n",
      " [ 0.04026053 -0.16236177 -0.22981994]\n",
      " [ 0.70999974  0.78137213 -0.301157  ]]\n",
      "173 0.607686 [[-1.244535   -0.9283856   0.91474485]\n",
      " [ 0.04046011 -0.16202621 -0.23035507]\n",
      " [ 0.7116038   0.78062546 -0.30201444]]\n",
      "174 0.6069586 [[-1.2492118  -0.9268465   0.9178826 ]\n",
      " [ 0.04065667 -0.16169444 -0.2308834 ]\n",
      " [ 0.7132066   0.77988267 -0.30287442]]\n",
      "175 0.6062349 [[-1.2538772  -0.9253108   0.92101234]\n",
      " [ 0.04084972 -0.1613658  -0.23140506]\n",
      " [ 0.71480757  0.77914417 -0.30373687]]\n",
      "176 0.60551465 [[-1.2585311  -0.92377853  0.9241341 ]\n",
      " [ 0.04103974 -0.16104072 -0.23192015]\n",
      " [ 0.7164071   0.7784095  -0.30460173]]\n",
      "177 0.60479796 [[-1.2631738  -0.9222497   0.92724794]\n",
      " [ 0.04122645 -0.16071881 -0.23242877]\n",
      " [ 0.71800494  0.77767885 -0.30546892]]\n",
      "178 0.60408473 [[-1.2678052  -0.9207243   0.93035394]\n",
      " [ 0.04141012 -0.16040023 -0.23293103]\n",
      " [ 0.7196013   0.77695197 -0.30633837]]\n",
      "179 0.60337484 [[-1.2724255  -0.91920227  0.9334522 ]\n",
      " [ 0.04159058 -0.1600847  -0.23342703]\n",
      " [ 0.72119594  0.77622896 -0.30721003]]\n",
      "180 0.6026684 [[-1.2770348  -0.9176837   0.9365428 ]\n",
      " [ 0.04176806 -0.15977235 -0.23391688]\n",
      " [ 0.72278905  0.77550966 -0.30808383]]\n",
      "181 0.6019653 [[-1.2816329  -0.9161685   0.9396258 ]\n",
      " [ 0.04194244 -0.15946294 -0.23440066]\n",
      " [ 0.7243805   0.77479416 -0.30895975]]\n",
      "182 0.60126555 [[-1.2862202  -0.91465676  0.9427013 ]\n",
      " [ 0.04211393 -0.15915662 -0.23487847]\n",
      " [ 0.7259704   0.7740822  -0.30983767]]\n",
      "183 0.60056907 [[-1.2907966  -0.91314834  0.94576925]\n",
      " [ 0.04228237 -0.15885313 -0.2353504 ]\n",
      " [ 0.7275586   0.7733739  -0.31071755]]\n",
      "184 0.5998758 [[-1.2953622  -0.9116433   0.9488298 ]\n",
      " [ 0.04244801 -0.15855263 -0.23581654]\n",
      " [ 0.7291453   0.772669   -0.31159934]]\n",
      "185 0.59918576 [[-1.2999172  -0.91014165  0.9518831 ]\n",
      " [ 0.04261064 -0.15825482 -0.23627698]\n",
      " [ 0.73073024  0.7719677  -0.31248298]]\n",
      "186 0.598499 [[-1.3044615  -0.90864336  0.95492905]\n",
      " [ 0.04277056 -0.15795988 -0.23673183]\n",
      " [ 0.73231363  0.77126974 -0.3133684 ]]\n",
      "187 0.5978153 [[-1.3089952  -0.90714836  0.9579678 ]\n",
      " [ 0.04292757 -0.15766756 -0.23718116]\n",
      " [ 0.7338953   0.7705753  -0.3142556 ]]\n",
      "188 0.5971347 [[-1.3135185  -0.90565675  0.9609994 ]\n",
      " [ 0.04308194 -0.15737803 -0.23762505]\n",
      " [ 0.7354754   0.769884   -0.31514448]]\n",
      "189 0.59645736 [[-1.3180314  -0.9041684   0.96402395]\n",
      " [ 0.04323345 -0.157091   -0.23806359]\n",
      " [ 0.7370538   0.76919615 -0.316035  ]]\n",
      "190 0.595783 [[-1.322534   -0.90268344  0.9670415 ]\n",
      " [ 0.0433824  -0.15680669 -0.23849687]\n",
      " [ 0.7386307   0.7685114  -0.31692713]]\n",
      "191 0.5951117 [[-1.3270262  -0.9012018   0.97005206]\n",
      " [ 0.04352855 -0.15652475 -0.23892497]\n",
      " [ 0.74020576  0.76783    -0.3178208 ]]\n",
      "192 0.59444344 [[-1.3315083  -0.8997234   0.9730557 ]\n",
      " [ 0.04367223 -0.15624544 -0.23934795]\n",
      " [ 0.7417793   0.7671516  -0.31871596]]\n",
      "193 0.593778 [[-1.3359802  -0.8982483   0.9760525 ]\n",
      " [ 0.04381314 -0.1559684  -0.23976591]\n",
      " [ 0.74335104  0.76647645 -0.31961256]]\n",
      "194 0.5931156 [[-1.340442   -0.89677656  0.9790426 ]\n",
      " [ 0.04395167 -0.15569393 -0.24017891]\n",
      " [ 0.7449213   0.76580423 -0.32051057]]\n",
      "195 0.5924562 [[-1.3448938  -0.895308    0.9820259 ]\n",
      " [ 0.04408749 -0.15542161 -0.24058704]\n",
      " [ 0.7464897   0.7651352  -0.32140994]]\n",
      "196 0.5917996 [[-1.3493357  -0.8938428   0.9850026 ]\n",
      " [ 0.04422105 -0.15515186 -0.24099036]\n",
      " [ 0.74805665  0.76446897 -0.32231063]]\n",
      "197 0.591146 [[-1.3537678  -0.8923808   0.9879726 ]\n",
      " [ 0.04435192 -0.15488413 -0.24138895]\n",
      " [ 0.7496217   0.76380587 -0.3232126 ]]\n",
      "198 0.5904952 [[-1.35819    -0.89092207  0.9909361 ]\n",
      " [ 0.04448061 -0.15461889 -0.24178287]\n",
      " [ 0.7511853   0.76314545 -0.32411578]]\n",
      "199 0.58984715 [[-1.3626025  -0.8894665   0.9938931 ]\n",
      " [ 0.04460664 -0.15435556 -0.24217223]\n",
      " [ 0.75274694  0.7624882  -0.3250202 ]]\n",
      "200 0.589202 [[-1.3670053  -0.88801426  0.99684364]\n",
      " [ 0.04473059 -0.15409467 -0.24255705]\n",
      " [ 0.7543072   0.7618335  -0.32592574]]\n",
      "Prediction: [2 2 2]\n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "tf.set_random_seed(777) # for reproducibility\n",
    "\n",
    "x_data = [[1, 2, 1],\n",
    "          [1, 3, 2],\n",
    "          [1, 3, 4],\n",
    "          [1, 5, 5],\n",
    "          [1, 7, 5],\n",
    "          [1, 2, 5],\n",
    "          [1, 6, 6],\n",
    "          [1, 7, 7]]\n",
    "y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]\n",
    "\n",
    "# Evaluation our model using this test dataset\n",
    "x_test = [[2,1,1],\n",
    "         [3,1,2],\n",
    "         [3,3,4]]\n",
    "y_test = [[0,0,1],\n",
    "         [0,0,1],\n",
    "         [0,0,1]]\n",
    "\n",
    "X = tf.placeholder(\"float\",[None,3])\n",
    "Y = tf.placeholder(\"float\",[None,3])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3,3]))\n",
    "b = tf.Variable(tf.random_normal([3]))\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduct_sum(exp(logits),dim)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X,W)+b)\n",
    "\n",
    "# Cross entropy cost/loss\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(hypothesis),axis=1))\n",
    "\n",
    "# Try to change learning_rate to small numbers\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Coreect prediction Test model\n",
    "prediction = tf.argmax(hypothesis,1)\n",
    "is_correct = tf.equal(prediction, tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct,tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(201):\n",
    "        cost_val, W_val, _ = sess.run([cost,W,optimizer],feed_dict={X:x_data, Y:y_data})\n",
    "        print(step, cost_val, W_val)\n",
    "        \n",
    "    # predict\n",
    "    print(\"Prediction:\",sess.run(prediction, feed_dict={X:x_test}))\n",
    "    \n",
    "    # Calculate the accuracy\n",
    "    print(\"Accuracy: \",sess.run(accuracy, feed_dict={X:x_test,Y:y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Big learning rate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.309568 [[ 1.362457   -1.0471004   1.0119414 ]\n",
      " [ 2.1537383  -1.5007653   0.5748935 ]\n",
      " [ 1.0664344  -3.665341    0.93397427]]\n",
      "1 19.70678 [[ 0.24111295 -0.48460042  1.5707855 ]\n",
      " [-1.9632957   1.1242347   2.0669277 ]\n",
      " [-3.0492082  -0.8528409   2.2371168 ]]\n",
      "2 24.873043 [[ 0.6161129   0.07788789  0.6332972 ]\n",
      " [ 0.47420406  3.749211   -2.9955485 ]\n",
      " [-0.61170816  1.9596472  -3.0128713 ]]\n",
      "3 19.378582 [[ 0.9909121  -0.8594113   1.1957972 ]\n",
      " [ 2.9113016  -0.18788648 -1.4955485 ]\n",
      " [ 1.8255906  -1.7901516  -1.7003713 ]]\n",
      "4 21.601315 [[-0.1340872  -0.29691142  1.7582965 ]\n",
      " [-1.213697    2.4371133   0.00445008]\n",
      " [-2.2994084   1.0223484  -0.38787198]]\n",
      "5 12.867058 [[ 0.24089539 -1.2005079   2.2869103 ]\n",
      " [ 1.2237682  -1.4316785   1.435777  ]\n",
      " [ 0.13807416 -2.6921802   0.88917434]]\n",
      "6 17.022417 [[ 0.6131735  -0.6380079   1.3521322 ]\n",
      " [ 3.6546245   1.1933215  -3.6200788 ]\n",
      " [ 2.5709715   0.12031984 -4.356223  ]]\n",
      "7 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "8 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "9 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "10 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "11 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "12 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "13 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "14 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "15 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "16 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "17 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "18 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "19 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "20 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "21 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "22 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "23 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "24 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "25 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "26 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "27 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "28 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "29 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "30 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "31 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "32 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "33 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "34 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "35 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "36 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "37 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "38 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "39 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "40 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "41 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "42 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "43 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "44 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "45 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "46 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "47 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "48 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "49 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "50 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "51 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "52 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "53 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "54 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "55 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "56 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "57 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "58 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "59 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "60 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "61 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "62 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "63 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "64 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "65 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "66 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "67 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "68 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "69 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "70 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "71 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "72 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "73 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "74 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "75 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "76 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "77 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "78 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "79 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "80 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "81 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "82 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "83 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "84 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "85 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "86 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "87 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "88 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "89 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "90 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "91 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "92 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "93 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "94 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "95 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "96 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "97 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "98 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "99 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "100 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "101 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "102 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "103 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "104 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "105 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "106 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "107 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "108 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "109 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "110 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "111 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "112 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "113 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "114 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "115 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "116 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "117 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "118 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "119 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "120 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "121 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "122 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "123 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "124 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "125 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "126 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "127 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "128 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "129 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "130 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "131 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "132 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "133 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "134 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "135 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "136 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "137 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "138 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "139 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "140 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "141 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "142 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "143 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "144 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "145 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "146 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "147 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "148 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "149 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "150 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "151 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "152 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "154 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "155 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "156 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "157 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "158 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "159 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "160 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "161 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "162 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "163 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "164 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "165 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "166 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "167 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "168 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "169 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "170 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "171 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "172 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "173 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "174 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "175 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "176 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "177 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "178 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "179 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "180 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "181 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "182 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "183 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "184 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "185 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "186 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "187 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "188 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "189 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "190 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "191 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "192 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "193 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "194 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "195 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "196 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "197 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "198 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "199 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "200 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "Prediction: [0 0 0]\n",
      "Accuracy:  0.0\n"
     ]
    }
   ],
   "source": [
    "tf.set_random_seed(777) # for reproducibility\n",
    "\n",
    "x_data = [[1, 2, 1],\n",
    "          [1, 3, 2],\n",
    "          [1, 3, 4],\n",
    "          [1, 5, 5],\n",
    "          [1, 7, 5],\n",
    "          [1, 2, 5],\n",
    "          [1, 6, 6],\n",
    "          [1, 7, 7]]\n",
    "y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]\n",
    "\n",
    "# Evaluation our model using this test dataset\n",
    "x_test = [[2,1,1],\n",
    "         [3,1,2],\n",
    "         [3,3,4]]\n",
    "y_test = [[0,0,1],\n",
    "         [0,0,1],\n",
    "         [0,0,1]]\n",
    "\n",
    "X = tf.placeholder(\"float\",[None,3])\n",
    "Y = tf.placeholder(\"float\",[None,3])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3,3]))\n",
    "b = tf.Variable(tf.random_normal([3]))\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduct_sum(exp(logits),dim)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X,W)+b)\n",
    "\n",
    "# Cross entropy cost/loss\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(hypothesis),axis=1))\n",
    "\n",
    "# Try to change learning_rate to small numbers\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.5).minimize(cost) # Big learning rate\n",
    "\n",
    "# Coreect prediction Test model\n",
    "prediction = tf.argmax(hypothesis,1)\n",
    "is_correct = tf.equal(prediction, tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct,tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(201):\n",
    "        cost_val, W_val, _ = sess.run([cost,W,optimizer],feed_dict={X:x_data, Y:y_data})\n",
    "        print(step, cost_val, W_val)\n",
    "        \n",
    "    # predict\n",
    "    print(\"Prediction:\",sess.run(prediction, feed_dict={X:x_test}))\n",
    "    \n",
    "    # Calculate the accuracy\n",
    "    print(\"Accuracy: \",sess.run(accuracy, feed_dict={X:x_test,Y:y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Small learning rate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "1 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "2 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "3 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "4 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "5 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "6 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "7 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "8 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "9 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "10 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "11 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "12 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "13 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "14 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "15 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "16 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "17 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "18 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "19 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "20 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "21 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "22 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "23 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "24 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "25 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "26 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "27 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "28 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "29 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "30 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "31 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "32 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "33 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "34 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "35 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "36 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "37 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "38 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "39 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "40 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "41 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "42 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "43 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "44 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "45 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "46 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "47 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "48 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "49 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "50 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "51 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "52 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "53 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "54 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "55 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "56 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "57 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "58 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "59 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "60 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "61 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "62 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "63 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "64 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "65 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "66 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "67 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "68 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "69 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "70 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "71 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "72 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "73 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "74 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "75 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "76 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "77 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "79 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "80 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "81 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "82 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "83 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "84 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "85 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "86 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "87 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "88 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "89 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "90 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "91 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "92 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "93 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "94 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "95 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "96 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "97 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "98 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "99 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "100 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "101 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "102 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "103 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "104 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "105 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "106 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "107 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "108 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "109 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "110 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "111 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "112 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "113 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "114 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "115 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "116 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "117 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "118 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "119 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "120 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "121 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "122 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "123 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "124 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "125 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "126 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "127 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "128 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "129 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "130 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "131 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "132 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "133 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "134 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "135 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "136 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "137 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "138 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "139 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "140 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "141 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "142 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "143 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "144 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "145 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "146 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "147 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "148 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "149 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "150 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "151 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "152 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "153 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "154 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "155 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "156 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "157 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "159 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "160 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "161 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "162 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "163 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "164 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "165 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "166 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "167 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "168 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "169 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "170 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "171 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "172 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "173 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "174 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "175 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "176 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "177 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "178 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "179 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "180 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "181 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "182 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "183 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "184 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "185 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "186 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "187 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "188 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "189 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "190 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "191 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "192 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "193 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "194 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "195 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "196 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "197 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "198 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "199 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "200 5.9145193 [[-2.147858    0.41738197 -1.9090626 ]\n",
      " [-0.9709558   0.7049217   0.5887852 ]\n",
      " [-1.4944313  -0.80274045 -1.2553881 ]]\n",
      "Prediction: [1 1 1]\n",
      "Accuracy:  0.0\n"
     ]
    }
   ],
   "source": [
    "tf.set_random_seed(777) # for reproducibility\n",
    "\n",
    "x_data = [[1, 2, 1],\n",
    "          [1, 3, 2],\n",
    "          [1, 3, 4],\n",
    "          [1, 5, 5],\n",
    "          [1, 7, 5],\n",
    "          [1, 2, 5],\n",
    "          [1, 6, 6],\n",
    "          [1, 7, 7]]\n",
    "y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]\n",
    "\n",
    "# Evaluation our model using this test dataset\n",
    "x_test = [[2,1,1],\n",
    "         [3,1,2],\n",
    "         [3,3,4]]\n",
    "y_test = [[0,0,1],\n",
    "         [0,0,1],\n",
    "         [0,0,1]]\n",
    "\n",
    "X = tf.placeholder(\"float\",[None,3])\n",
    "Y = tf.placeholder(\"float\",[None,3])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3,3]))\n",
    "b = tf.Variable(tf.random_normal([3]))\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduct_sum(exp(logits),dim)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X,W)+b)\n",
    "\n",
    "# Cross entropy cost/loss\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(hypothesis),axis=1))\n",
    "\n",
    "# Try to change learning_rate to small numbers\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-10).minimize(cost) # Small learning rate\n",
    "\n",
    "# Coreect prediction Test model\n",
    "prediction = tf.argmax(hypothesis,1)\n",
    "is_correct = tf.equal(prediction, tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct,tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(201):\n",
    "        cost_val, W_val, _ = sess.run([cost,W,optimizer],feed_dict={X:x_data, Y:y_data})\n",
    "        print(step, cost_val, W_val)\n",
    "        \n",
    "    # predict\n",
    "    print(\"Prediction:\",sess.run(prediction, feed_dict={X:x_test}))\n",
    "    \n",
    "    # Calculate the accuracy\n",
    "    print(\"Accuracy: \",sess.run(accuracy, feed_dict={X:x_test,Y:y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lab 07-2 linear regression without min max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Non-normalized inputs : xy -> 잘 학습 X**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  1874542900000.0 \n",
      "Prediction:\n",
      " [[ -964430.3]\n",
      " [-1944005.2]\n",
      " [-1528768.1]\n",
      " [-1070941.1]\n",
      " [-1262587.5]\n",
      " [-1273236.1]\n",
      " [-1166784.5]\n",
      " [-1486204.6]]\n",
      "1 Cost:  2.0595241e+27 \n",
      "Prediction:\n",
      " [[3.2012142e+13]\n",
      " [6.4443724e+13]\n",
      " [5.0695554e+13]\n",
      " [3.5537314e+13]\n",
      " [4.1882621e+13]\n",
      " [4.2235140e+13]\n",
      " [3.8709970e+13]\n",
      " [4.9285484e+13]]\n",
      "2 Cost:  inf \n",
      "Prediction:\n",
      " [[-1.0610856e+21]\n",
      " [-2.1360741e+21]\n",
      " [-1.6803724e+21]\n",
      " [-1.1779320e+21]\n",
      " [-1.3882558e+21]\n",
      " [-1.3999404e+21]\n",
      " [-1.2830938e+21]\n",
      " [-1.6336336e+21]]\n",
      "3 Cost:  inf \n",
      "Prediction:\n",
      " [[3.5171109e+28]\n",
      " [7.0803050e+28]\n",
      " [5.5698202e+28]\n",
      " [3.9044146e+28]\n",
      " [4.6015613e+28]\n",
      " [4.6402913e+28]\n",
      " [4.2529878e+28]\n",
      " [5.4148987e+28]]\n",
      "4 Cost:  inf \n",
      "Prediction:\n",
      " [[-1.16579392e+36]\n",
      " [-2.34686256e+36]\n",
      " [-1.84619207e+36]\n",
      " [-1.29417096e+36]\n",
      " [-1.52524956e+36]\n",
      " [-1.53808722e+36]\n",
      " [-1.40971018e+36]\n",
      " [-1.79484129e+36]]\n",
      "5 Cost:  inf \n",
      "Prediction:\n",
      " [[inf]\n",
      " [inf]\n",
      " [inf]\n",
      " [inf]\n",
      " [inf]\n",
      " [inf]\n",
      " [inf]\n",
      " [inf]]\n",
      "6 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "7 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "8 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "9 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "10 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "11 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "12 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "13 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "14 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "15 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "16 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "17 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "18 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "19 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "20 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "21 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "22 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "23 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "24 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "25 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "26 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "27 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "28 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "29 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "30 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "31 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "32 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "33 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "34 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "35 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "36 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "37 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "38 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "39 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "40 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "41 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "42 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "43 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "44 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "45 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "46 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "47 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "48 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "49 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "50 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "51 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "52 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "53 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "54 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "55 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "56 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "57 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "58 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "59 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "60 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "61 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "62 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "63 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "64 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "65 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "66 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "67 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "68 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "69 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "70 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "71 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "72 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "73 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "74 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "75 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "76 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "77 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "78 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "79 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "80 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "81 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "82 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "83 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "84 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "85 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "86 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "87 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "88 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "89 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "90 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "91 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "92 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "93 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "94 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "95 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "96 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "97 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "98 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "99 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "100 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.set_random_seed(777) # for reproducibility\n",
    "\n",
    "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "               [819, 823, 1198100, 816, 820.450012],\n",
    "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])\n",
    "\n",
    "x_data = xy[:,0:-1]\n",
    "y_data = xy[:,[-1]]\n",
    "\n",
    "# placeholders for a tensor that will be always fed\n",
    "X = tf.placeholder(tf.float32, shape=[None,4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4,1]),name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]),name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "hypothesis = tf.matmul(X,W)+b\n",
    "\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Initializes global variables in the graph\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(101):\n",
    "    cost_val, hy_val, _ = sess.run([cost,hypothesis,train],feed_dict={X:x_data, Y:y_data})\n",
    "    print(step,\"Cost: \",cost_val, \"\\nPrediction:\\n\",hy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lab 07-3 linear regression min max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalized inputs(min-max scale) -> 잘 학습**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99999999 0.99999999 0.         1.         1.        ]\n",
      " [0.70548491 0.70439552 1.         0.71881782 0.83755791]\n",
      " [0.54412549 0.50274824 0.57608696 0.606468   0.6606331 ]\n",
      " [0.33890353 0.31368023 0.10869565 0.45989134 0.43800918]\n",
      " [0.51436    0.42582389 0.30434783 0.58504805 0.42624401]\n",
      " [0.49556179 0.42582389 0.31521739 0.48131134 0.49276137]\n",
      " [0.11436064 0.         0.20652174 0.22007776 0.18597238]\n",
      " [0.         0.07747099 0.5326087  0.         0.        ]]\n",
      "0 Cost:  0.15811011 \n",
      "Prediction:\n",
      " [[ 1.1884168 ]\n",
      " [ 1.7239391 ]\n",
      " [ 0.98022044]\n",
      " [ 0.18211696]\n",
      " [ 0.6094302 ]\n",
      " [ 0.68686604]\n",
      " [-0.13890897]\n",
      " [ 0.3151348 ]]\n",
      "1 Cost:  0.1581056 \n",
      "Prediction:\n",
      " [[ 1.1884052 ]\n",
      " [ 1.7239264 ]\n",
      " [ 0.9802104 ]\n",
      " [ 0.18210989]\n",
      " [ 0.6094215 ]\n",
      " [ 0.6868576 ]\n",
      " [-0.13891424]\n",
      " [ 0.31512907]]\n",
      "2 Cost:  0.15810111 \n",
      "Prediction:\n",
      " [[ 1.1883937 ]\n",
      " [ 1.7239139 ]\n",
      " [ 0.9802005 ]\n",
      " [ 0.18210289]\n",
      " [ 0.6094128 ]\n",
      " [ 0.68684924]\n",
      " [-0.1389195 ]\n",
      " [ 0.31512347]]\n",
      "3 Cost:  0.15809658 \n",
      "Prediction:\n",
      " [[ 1.1883819 ]\n",
      " [ 1.7239013 ]\n",
      " [ 0.98019063]\n",
      " [ 0.18209589]\n",
      " [ 0.6094041 ]\n",
      " [ 0.6868408 ]\n",
      " [-0.13892478]\n",
      " [ 0.3151178 ]]\n",
      "4 Cost:  0.1580921 \n",
      "Prediction:\n",
      " [[ 1.1883701 ]\n",
      " [ 1.7238888 ]\n",
      " [ 0.98018074]\n",
      " [ 0.18208882]\n",
      " [ 0.6093954 ]\n",
      " [ 0.6868322 ]\n",
      " [-0.13893005]\n",
      " [ 0.3151122 ]]\n",
      "5 Cost:  0.15808758 \n",
      "Prediction:\n",
      " [[ 1.1883584 ]\n",
      " [ 1.7238761 ]\n",
      " [ 0.9801706 ]\n",
      " [ 0.18208173]\n",
      " [ 0.6093867 ]\n",
      " [ 0.68682384]\n",
      " [-0.13893533]\n",
      " [ 0.31510654]]\n",
      "6 Cost:  0.15808308 \n",
      "Prediction:\n",
      " [[ 1.188347  ]\n",
      " [ 1.7238636 ]\n",
      " [ 0.98016083]\n",
      " [ 0.18207482]\n",
      " [ 0.60937786]\n",
      " [ 0.6868154 ]\n",
      " [-0.13894059]\n",
      " [ 0.31510082]]\n",
      "7 Cost:  0.15807861 \n",
      "Prediction:\n",
      " [[ 1.1883352 ]\n",
      " [ 1.7238512 ]\n",
      " [ 0.98015094]\n",
      " [ 0.18206772]\n",
      " [ 0.60936916]\n",
      " [ 0.6868069 ]\n",
      " [-0.13894585]\n",
      " [ 0.31509522]]\n",
      "8 Cost:  0.15807414 \n",
      "Prediction:\n",
      " [[ 1.1883234 ]\n",
      " [ 1.7238387 ]\n",
      " [ 0.98014104]\n",
      " [ 0.18206066]\n",
      " [ 0.60936046]\n",
      " [ 0.6867986 ]\n",
      " [-0.13895114]\n",
      " [ 0.31508955]]\n",
      "9 Cost:  0.1580696 \n",
      "Prediction:\n",
      " [[ 1.1883117 ]\n",
      " [ 1.723826  ]\n",
      " [ 0.98013103]\n",
      " [ 0.18205366]\n",
      " [ 0.60935163]\n",
      " [ 0.68679   ]\n",
      " [-0.13895641]\n",
      " [ 0.31508395]]\n",
      "10 Cost:  0.15806511 \n",
      "Prediction:\n",
      " [[ 1.1883003 ]\n",
      " [ 1.7238135 ]\n",
      " [ 0.980121  ]\n",
      " [ 0.18204665]\n",
      " [ 0.60934305]\n",
      " [ 0.6867815 ]\n",
      " [-0.13896169]\n",
      " [ 0.31507823]]\n",
      "11 Cost:  0.15806058 \n",
      "Prediction:\n",
      " [[ 1.1882885 ]\n",
      " [ 1.7238009 ]\n",
      " [ 0.9801111 ]\n",
      " [ 0.18203965]\n",
      " [ 0.60933423]\n",
      " [ 0.6867732 ]\n",
      " [-0.13896695]\n",
      " [ 0.31507257]]\n",
      "12 Cost:  0.15805608 \n",
      "Prediction:\n",
      " [[ 1.1882766 ]\n",
      " [ 1.7237884 ]\n",
      " [ 0.9801012 ]\n",
      " [ 0.18203264]\n",
      " [ 0.6093255 ]\n",
      " [ 0.6867647 ]\n",
      " [-0.13897221]\n",
      " [ 0.31506696]]\n",
      "13 Cost:  0.15805155 \n",
      "Prediction:\n",
      " [[ 1.188265  ]\n",
      " [ 1.7237757 ]\n",
      " [ 0.9800912 ]\n",
      " [ 0.18202558]\n",
      " [ 0.6093167 ]\n",
      " [ 0.68675613]\n",
      " [-0.13897748]\n",
      " [ 0.3150613 ]]\n",
      "14 Cost:  0.15804714 \n",
      "Prediction:\n",
      " [[ 1.1882535 ]\n",
      " [ 1.7237635 ]\n",
      " [ 0.98008144]\n",
      " [ 0.18201852]\n",
      " [ 0.609308  ]\n",
      " [ 0.68674767]\n",
      " [-0.13898276]\n",
      " [ 0.3150557 ]]\n",
      "15 Cost:  0.15804262 \n",
      "Prediction:\n",
      " [[ 1.1882417 ]\n",
      " [ 1.7237508 ]\n",
      " [ 0.98007154]\n",
      " [ 0.18201151]\n",
      " [ 0.6092993 ]\n",
      " [ 0.6867393 ]\n",
      " [-0.13898803]\n",
      " [ 0.31504998]]\n",
      "16 Cost:  0.15803811 \n",
      "Prediction:\n",
      " [[ 1.1882299 ]\n",
      " [ 1.7237383 ]\n",
      " [ 0.98006153]\n",
      " [ 0.18200448]\n",
      " [ 0.6092905 ]\n",
      " [ 0.68673086]\n",
      " [-0.13899331]\n",
      " [ 0.31504437]]\n",
      "17 Cost:  0.1580336 \n",
      "Prediction:\n",
      " [[ 1.1882182 ]\n",
      " [ 1.7237257 ]\n",
      " [ 0.9800515 ]\n",
      " [ 0.18199751]\n",
      " [ 0.6092818 ]\n",
      " [ 0.6867224 ]\n",
      " [-0.13899858]\n",
      " [ 0.3150387 ]]\n",
      "18 Cost:  0.15802917 \n",
      "Prediction:\n",
      " [[ 1.1882068 ]\n",
      " [ 1.7237134 ]\n",
      " [ 0.9800415 ]\n",
      " [ 0.18199047]\n",
      " [ 0.6092731 ]\n",
      " [ 0.68671393]\n",
      " [-0.13900386]\n",
      " [ 0.3150331 ]]\n",
      "19 Cost:  0.15802458 \n",
      "Prediction:\n",
      " [[ 1.188195  ]\n",
      " [ 1.7237005 ]\n",
      " [ 0.9800317 ]\n",
      " [ 0.18198344]\n",
      " [ 0.60926425]\n",
      " [ 0.68670547]\n",
      " [-0.13900912]\n",
      " [ 0.3150274 ]]\n",
      "20 Cost:  0.15802015 \n",
      "Prediction:\n",
      " [[ 1.1881832 ]\n",
      " [ 1.7236882 ]\n",
      " [ 0.98002183]\n",
      " [ 0.18197635]\n",
      " [ 0.60925555]\n",
      " [ 0.6866971 ]\n",
      " [-0.13901438]\n",
      " [ 0.31502172]]\n",
      "21 Cost:  0.15801564 \n",
      "Prediction:\n",
      " [[ 1.1881715 ]\n",
      " [ 1.7236756 ]\n",
      " [ 0.9800118 ]\n",
      " [ 0.18196937]\n",
      " [ 0.60924685]\n",
      " [ 0.68668866]\n",
      " [-0.13901965]\n",
      " [ 0.31501612]]\n",
      "22 Cost:  0.15801117 \n",
      "Prediction:\n",
      " [[ 1.1881601 ]\n",
      " [ 1.7236631 ]\n",
      " [ 0.9800019 ]\n",
      " [ 0.18196234]\n",
      " [ 0.60923815]\n",
      " [ 0.6866802 ]\n",
      " [-0.13902493]\n",
      " [ 0.31501046]]\n",
      "23 Cost:  0.15800667 \n",
      "Prediction:\n",
      " [[ 1.1881483 ]\n",
      " [ 1.7236507 ]\n",
      " [ 0.9799919 ]\n",
      " [ 0.18195531]\n",
      " [ 0.60922945]\n",
      " [ 0.6866716 ]\n",
      " [-0.13903022]\n",
      " [ 0.3150048 ]]\n",
      "24 Cost:  0.15800214 \n",
      "Prediction:\n",
      " [[ 1.1881365 ]\n",
      " [ 1.7236379 ]\n",
      " [ 0.97998214]\n",
      " [ 0.18194833]\n",
      " [ 0.60922074]\n",
      " [ 0.68666327]\n",
      " [-0.1390355 ]\n",
      " [ 0.31499913]]\n",
      "25 Cost:  0.15799765 \n",
      "Prediction:\n",
      " [[ 1.1881248 ]\n",
      " [ 1.7236255 ]\n",
      " [ 0.979972  ]\n",
      " [ 0.18194124]\n",
      " [ 0.6092118 ]\n",
      " [ 0.6866548 ]\n",
      " [-0.13904075]\n",
      " [ 0.31499353]]\n",
      "26 Cost:  0.15799314 \n",
      "Prediction:\n",
      " [[ 1.1881133 ]\n",
      " [ 1.7236128 ]\n",
      " [ 0.9799621 ]\n",
      " [ 0.18193421]\n",
      " [ 0.60920334]\n",
      " [ 0.68664634]\n",
      " [-0.13904601]\n",
      " [ 0.31498787]]\n",
      "27 Cost:  0.15798864 \n",
      "Prediction:\n",
      " [[ 1.1881015 ]\n",
      " [ 1.7236004 ]\n",
      " [ 0.9799521 ]\n",
      " [ 0.18192717]\n",
      " [ 0.6091944 ]\n",
      " [ 0.6866379 ]\n",
      " [-0.13905129]\n",
      " [ 0.3149822 ]]\n",
      "28 Cost:  0.15798411 \n",
      "Prediction:\n",
      " [[ 1.1880897 ]\n",
      " [ 1.7235876 ]\n",
      " [ 0.9799422 ]\n",
      " [ 0.1819202 ]\n",
      " [ 0.6091857 ]\n",
      " [ 0.68662953]\n",
      " [-0.13905656]\n",
      " [ 0.31497654]]\n",
      "29 Cost:  0.15797964 \n",
      "Prediction:\n",
      " [[ 1.188078  ]\n",
      " [ 1.7235752 ]\n",
      " [ 0.9799323 ]\n",
      " [ 0.18191311]\n",
      " [ 0.609177  ]\n",
      " [ 0.68662095]\n",
      " [-0.13906184]\n",
      " [ 0.31497088]]\n",
      "30 Cost:  0.15797523 \n",
      "Prediction:\n",
      " [[ 1.1880666 ]\n",
      " [ 1.723563  ]\n",
      " [ 0.9799224 ]\n",
      " [ 0.18190613]\n",
      " [ 0.6091683 ]\n",
      " [ 0.6866125 ]\n",
      " [-0.1390671 ]\n",
      " [ 0.31496528]]\n",
      "31 Cost:  0.15797071 \n",
      "Prediction:\n",
      " [[ 1.1880548 ]\n",
      " [ 1.7235503 ]\n",
      " [ 0.9799125 ]\n",
      " [ 0.1818991 ]\n",
      " [ 0.6091596 ]\n",
      " [ 0.68660414]\n",
      " [-0.13907239]\n",
      " [ 0.31495962]]\n",
      "32 Cost:  0.15796623 \n",
      "Prediction:\n",
      " [[ 1.188043  ]\n",
      " [ 1.7235378 ]\n",
      " [ 0.9799026 ]\n",
      " [ 0.18189207]\n",
      " [ 0.6091509 ]\n",
      " [ 0.6865957 ]\n",
      " [-0.13907766]\n",
      " [ 0.31495395]]\n",
      "33 Cost:  0.15796168 \n",
      "Prediction:\n",
      " [[ 1.1880313 ]\n",
      " [ 1.7235252 ]\n",
      " [ 0.9798926 ]\n",
      " [ 0.18188503]\n",
      " [ 0.60914207]\n",
      " [ 0.6865871 ]\n",
      " [-0.13908292]\n",
      " [ 0.3149483 ]]\n",
      "34 Cost:  0.15795723 \n",
      "Prediction:\n",
      " [[ 1.1880199 ]\n",
      " [ 1.7235126 ]\n",
      " [ 0.9798827 ]\n",
      " [ 0.18187803]\n",
      " [ 0.6091335 ]\n",
      " [ 0.68657875]\n",
      " [-0.13908818]\n",
      " [ 0.3149427 ]]\n",
      "35 Cost:  0.15795268 \n",
      "Prediction:\n",
      " [[ 1.1880081 ]\n",
      " [ 1.7235    ]\n",
      " [ 0.9798727 ]\n",
      " [ 0.18187097]\n",
      " [ 0.60912466]\n",
      " [ 0.68657017]\n",
      " [-0.13909346]\n",
      " [ 0.31493703]]\n",
      "36 Cost:  0.1579482 \n",
      "Prediction:\n",
      " [[ 1.1879963 ]\n",
      " [ 1.7234875 ]\n",
      " [ 0.9798628 ]\n",
      " [ 0.18186396]\n",
      " [ 0.60911584]\n",
      " [ 0.6865618 ]\n",
      " [-0.13909873]\n",
      " [ 0.31493136]]\n",
      "37 Cost:  0.15794374 \n",
      "Prediction:\n",
      " [[ 1.1879846 ]\n",
      " [ 1.7234751 ]\n",
      " [ 0.9798529 ]\n",
      " [ 0.18185696]\n",
      " [ 0.60910726]\n",
      " [ 0.6865535 ]\n",
      " [-0.13910401]\n",
      " [ 0.3149257 ]]\n",
      "38 Cost:  0.15793926 \n",
      "Prediction:\n",
      " [[ 1.1879731 ]\n",
      " [ 1.7234626 ]\n",
      " [ 0.9798429 ]\n",
      " [ 0.1818499 ]\n",
      " [ 0.60909843]\n",
      " [ 0.686545  ]\n",
      " [-0.13910928]\n",
      " [ 0.31492004]]\n",
      "39 Cost:  0.15793473 \n",
      "Prediction:\n",
      " [[ 1.1879613 ]\n",
      " [ 1.72345   ]\n",
      " [ 0.9798329 ]\n",
      " [ 0.1818429 ]\n",
      " [ 0.6090896 ]\n",
      " [ 0.68653643]\n",
      " [-0.13911456]\n",
      " [ 0.31491444]]\n",
      "40 Cost:  0.15793024 \n",
      "Prediction:\n",
      " [[ 1.1879495 ]\n",
      " [ 1.7234374 ]\n",
      " [ 0.9798231 ]\n",
      " [ 0.18183589]\n",
      " [ 0.60908103]\n",
      " [ 0.68652797]\n",
      " [-0.13911982]\n",
      " [ 0.31490877]]\n",
      "41 Cost:  0.15792571 \n",
      "Prediction:\n",
      " [[ 1.1879379 ]\n",
      " [ 1.7234248 ]\n",
      " [ 0.979813  ]\n",
      " [ 0.18182883]\n",
      " [ 0.6090722 ]\n",
      " [ 0.6865196 ]\n",
      " [-0.1391251 ]\n",
      " [ 0.3149031 ]]\n",
      "42 Cost:  0.1579213 \n",
      "Prediction:\n",
      " [[ 1.1879264 ]\n",
      " [ 1.7234125 ]\n",
      " [ 0.9798032 ]\n",
      " [ 0.1818218 ]\n",
      " [ 0.6090635 ]\n",
      " [ 0.68651116]\n",
      " [-0.13913037]\n",
      " [ 0.31489745]]\n",
      "43 Cost:  0.15791672 \n",
      "Prediction:\n",
      " [[ 1.1879146 ]\n",
      " [ 1.7233996 ]\n",
      " [ 0.9797932 ]\n",
      " [ 0.18181482]\n",
      " [ 0.6090548 ]\n",
      " [ 0.6865026 ]\n",
      " [-0.13913563]\n",
      " [ 0.31489184]]\n",
      "44 Cost:  0.15791228 \n",
      "Prediction:\n",
      " [[ 1.1879028 ]\n",
      " [ 1.7233874 ]\n",
      " [ 0.9797832 ]\n",
      " [ 0.18180773]\n",
      " [ 0.6090461 ]\n",
      " [ 0.68649423]\n",
      " [-0.1391409 ]\n",
      " [ 0.31488618]]\n",
      "45 Cost:  0.15790775 \n",
      "Prediction:\n",
      " [[ 1.1878911 ]\n",
      " [ 1.7233747 ]\n",
      " [ 0.9797734 ]\n",
      " [ 0.18180078]\n",
      " [ 0.6090373 ]\n",
      " [ 0.68648577]\n",
      " [-0.13914618]\n",
      " [ 0.31488046]]\n",
      "46 Cost:  0.1579033 \n",
      "Prediction:\n",
      " [[ 1.1878797 ]\n",
      " [ 1.7233622 ]\n",
      " [ 0.9797635 ]\n",
      " [ 0.18179372]\n",
      " [ 0.6090286 ]\n",
      " [ 0.6864773 ]\n",
      " [-0.13915145]\n",
      " [ 0.31487486]]\n",
      "47 Cost:  0.15789875 \n",
      "Prediction:\n",
      " [[ 1.1878679 ]\n",
      " [ 1.7233496 ]\n",
      " [ 0.9797535 ]\n",
      " [ 0.18178666]\n",
      " [ 0.60901976]\n",
      " [ 0.6864687 ]\n",
      " [-0.13915673]\n",
      " [ 0.3148692 ]]\n",
      "48 Cost:  0.15789428 \n",
      "Prediction:\n",
      " [[ 1.1878561 ]\n",
      " [ 1.723337  ]\n",
      " [ 0.9797436 ]\n",
      " [ 0.18177965]\n",
      " [ 0.6090112 ]\n",
      " [ 0.6864605 ]\n",
      " [-0.13916199]\n",
      " [ 0.3148636 ]]\n",
      "49 Cost:  0.15788981 \n",
      "Prediction:\n",
      " [[ 1.1878444 ]\n",
      " [ 1.7233247 ]\n",
      " [ 0.9797336 ]\n",
      " [ 0.18177265]\n",
      " [ 0.60900235]\n",
      " [ 0.6864519 ]\n",
      " [-0.13916726]\n",
      " [ 0.31485793]]\n",
      "50 Cost:  0.15788534 \n",
      "Prediction:\n",
      " [[ 1.187833  ]\n",
      " [ 1.7233121 ]\n",
      " [ 0.9797238 ]\n",
      " [ 0.18176559]\n",
      " [ 0.60899353]\n",
      " [ 0.68644345]\n",
      " [-0.13917254]\n",
      " [ 0.3148522 ]]\n",
      "51 Cost:  0.15788081 \n",
      "Prediction:\n",
      " [[ 1.1878211 ]\n",
      " [ 1.7232995 ]\n",
      " [ 0.9797138 ]\n",
      " [ 0.18175858]\n",
      " [ 0.60898495]\n",
      " [ 0.6864351 ]\n",
      " [-0.13917781]\n",
      " [ 0.3148466 ]]\n",
      "52 Cost:  0.15787631 \n",
      "Prediction:\n",
      " [[ 1.1878093 ]\n",
      " [ 1.723287  ]\n",
      " [ 0.9797038 ]\n",
      " [ 0.18175158]\n",
      " [ 0.6089761 ]\n",
      " [ 0.68642664]\n",
      " [-0.13918309]\n",
      " [ 0.31484094]]\n",
      "53 Cost:  0.15787181 \n",
      "Prediction:\n",
      " [[ 1.1877977 ]\n",
      " [ 1.7232744 ]\n",
      " [ 0.9796939 ]\n",
      " [ 0.18174452]\n",
      " [ 0.60896754]\n",
      " [ 0.68641806]\n",
      " [-0.13918833]\n",
      " [ 0.31483534]]\n",
      "54 Cost:  0.15786736 \n",
      "Prediction:\n",
      " [[ 1.1877862 ]\n",
      " [ 1.7232618 ]\n",
      " [ 0.9796841 ]\n",
      " [ 0.18173748]\n",
      " [ 0.6089587 ]\n",
      " [ 0.68640983]\n",
      " [-0.13919361]\n",
      " [ 0.31482962]]\n",
      "55 Cost:  0.15786281 \n",
      "Prediction:\n",
      " [[ 1.1877744 ]\n",
      " [ 1.7232492 ]\n",
      " [ 0.979674  ]\n",
      " [ 0.18173051]\n",
      " [ 0.6089499 ]\n",
      " [ 0.68640125]\n",
      " [-0.1391989 ]\n",
      " [ 0.31482401]]\n",
      "56 Cost:  0.15785833 \n",
      "Prediction:\n",
      " [[ 1.1877626 ]\n",
      " [ 1.7232367 ]\n",
      " [ 0.9796641 ]\n",
      " [ 0.18172342]\n",
      " [ 0.6089413 ]\n",
      " [ 0.6863928 ]\n",
      " [-0.13920417]\n",
      " [ 0.31481835]]\n",
      "57 Cost:  0.15785386 \n",
      "Prediction:\n",
      " [[ 1.1877509 ]\n",
      " [ 1.7232243 ]\n",
      " [ 0.97965395]\n",
      " [ 0.18171644]\n",
      " [ 0.6089325 ]\n",
      " [ 0.68638444]\n",
      " [-0.13920943]\n",
      " [ 0.31481275]]\n",
      "58 Cost:  0.1578494 \n",
      "Prediction:\n",
      " [[ 1.1877395 ]\n",
      " [ 1.7232118 ]\n",
      " [ 0.9796443 ]\n",
      " [ 0.18170941]\n",
      " [ 0.6089238 ]\n",
      " [ 0.686376  ]\n",
      " [-0.13921471]\n",
      " [ 0.31480703]]\n",
      "59 Cost:  0.15784487 \n",
      "Prediction:\n",
      " [[ 1.1877277 ]\n",
      " [ 1.7231991 ]\n",
      " [ 0.9796343 ]\n",
      " [ 0.18170238]\n",
      " [ 0.6089151 ]\n",
      " [ 0.6863674 ]\n",
      " [-0.13921998]\n",
      " [ 0.31480137]]\n",
      "60 Cost:  0.15784039 \n",
      "Prediction:\n",
      " [[ 1.1877159 ]\n",
      " [ 1.7231866 ]\n",
      " [ 0.9796244 ]\n",
      " [ 0.18169534]\n",
      " [ 0.6089064 ]\n",
      " [ 0.6863589 ]\n",
      " [-0.13922524]\n",
      " [ 0.31479576]]\n",
      "61 Cost:  0.15783587 \n",
      "Prediction:\n",
      " [[ 1.1877042 ]\n",
      " [ 1.723174  ]\n",
      " [ 0.9796144 ]\n",
      " [ 0.18168837]\n",
      " [ 0.60889757]\n",
      " [ 0.6863506 ]\n",
      " [-0.13923052]\n",
      " [ 0.3147901 ]]\n",
      "62 Cost:  0.1578314 \n",
      "Prediction:\n",
      " [[ 1.1876928 ]\n",
      " [ 1.7231615 ]\n",
      " [ 0.9796045 ]\n",
      " [ 0.18168128]\n",
      " [ 0.608889  ]\n",
      " [ 0.6863421 ]\n",
      " [-0.13923578]\n",
      " [ 0.3147845 ]]\n",
      "63 Cost:  0.15782692 \n",
      "Prediction:\n",
      " [[ 1.187681  ]\n",
      " [ 1.7231491 ]\n",
      " [ 0.9795946 ]\n",
      " [ 0.1816743 ]\n",
      " [ 0.60888004]\n",
      " [ 0.68633354]\n",
      " [-0.13924107]\n",
      " [ 0.31477877]]\n",
      "64 Cost:  0.15782245 \n",
      "Prediction:\n",
      " [[ 1.1876692 ]\n",
      " [ 1.7231365 ]\n",
      " [ 0.9795847 ]\n",
      " [ 0.18166727]\n",
      " [ 0.60887146]\n",
      " [ 0.6863251 ]\n",
      " [-0.13924634]\n",
      " [ 0.31477317]]\n",
      "65 Cost:  0.15781793 \n",
      "Prediction:\n",
      " [[ 1.1876575 ]\n",
      " [ 1.7231239 ]\n",
      " [ 0.97957456]\n",
      " [ 0.1816602 ]\n",
      " [ 0.60886264]\n",
      " [ 0.6863167 ]\n",
      " [-0.13925162]\n",
      " [ 0.3147675 ]]\n",
      "66 Cost:  0.15781352 \n",
      "Prediction:\n",
      " [[ 1.187646  ]\n",
      " [ 1.7231116 ]\n",
      " [ 0.9795648 ]\n",
      " [ 0.1816532 ]\n",
      " [ 0.60885394]\n",
      " [ 0.6863084 ]\n",
      " [-0.13925688]\n",
      " [ 0.31476185]]\n",
      "67 Cost:  0.15780896 \n",
      "Prediction:\n",
      " [[ 1.1876342 ]\n",
      " [ 1.7230988 ]\n",
      " [ 0.9795549 ]\n",
      " [ 0.1816462 ]\n",
      " [ 0.60884523]\n",
      " [ 0.6862999 ]\n",
      " [-0.13926214]\n",
      " [ 0.31475618]]\n",
      "68 Cost:  0.15780452 \n",
      "Prediction:\n",
      " [[ 1.1876224 ]\n",
      " [ 1.7230865 ]\n",
      " [ 0.9795449 ]\n",
      " [ 0.18163913]\n",
      " [ 0.60883653]\n",
      " [ 0.68629146]\n",
      " [-0.13926741]\n",
      " [ 0.31475052]]\n",
      "69 Cost:  0.15779993 \n",
      "Prediction:\n",
      " [[ 1.1876107 ]\n",
      " [ 1.7230736 ]\n",
      " [ 0.97953486]\n",
      " [ 0.18163213]\n",
      " [ 0.6088277 ]\n",
      " [ 0.6862829 ]\n",
      " [-0.13927269]\n",
      " [ 0.31474492]]\n",
      "70 Cost:  0.15779552 \n",
      "Prediction:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [[ 1.1875993 ]\n",
      " [ 1.7230613 ]\n",
      " [ 0.9795251 ]\n",
      " [ 0.18162513]\n",
      " [ 0.608819  ]\n",
      " [ 0.6862744 ]\n",
      " [-0.13927798]\n",
      " [ 0.31473926]]\n",
      "71 Cost:  0.15779102 \n",
      "Prediction:\n",
      " [[ 1.1875875 ]\n",
      " [ 1.7230487 ]\n",
      " [ 0.9795151 ]\n",
      " [ 0.18161806]\n",
      " [ 0.6088103 ]\n",
      " [ 0.68626606]\n",
      " [-0.13928324]\n",
      " [ 0.31473365]]\n",
      "72 Cost:  0.15778652 \n",
      "Prediction:\n",
      " [[ 1.1875757 ]\n",
      " [ 1.7230362 ]\n",
      " [ 0.9795052 ]\n",
      " [ 0.181611  ]\n",
      " [ 0.6088016 ]\n",
      " [ 0.6862576 ]\n",
      " [-0.13928851]\n",
      " [ 0.31472793]]\n",
      "73 Cost:  0.15778206 \n",
      "Prediction:\n",
      " [[ 1.187564  ]\n",
      " [ 1.7230238 ]\n",
      " [ 0.97949517]\n",
      " [ 0.18160406]\n",
      " [ 0.6087928 ]\n",
      " [ 0.686249  ]\n",
      " [-0.13929379]\n",
      " [ 0.31472233]]\n",
      "74 Cost:  0.15777758 \n",
      "Prediction:\n",
      " [[ 1.1875526 ]\n",
      " [ 1.7230113 ]\n",
      " [ 0.9794853 ]\n",
      " [ 0.18159696]\n",
      " [ 0.6087841 ]\n",
      " [ 0.6862407 ]\n",
      " [-0.13929905]\n",
      " [ 0.31471667]]\n",
      "75 Cost:  0.15777306 \n",
      "Prediction:\n",
      " [[ 1.1875408 ]\n",
      " [ 1.7229986 ]\n",
      " [ 0.97947526]\n",
      " [ 0.18158999]\n",
      " [ 0.6087754 ]\n",
      " [ 0.6862322 ]\n",
      " [-0.13930432]\n",
      " [ 0.314711  ]]\n",
      "76 Cost:  0.15776858 \n",
      "Prediction:\n",
      " [[ 1.187529  ]\n",
      " [ 1.7229861 ]\n",
      " [ 0.9794655 ]\n",
      " [ 0.18158296]\n",
      " [ 0.60876656]\n",
      " [ 0.68622375]\n",
      " [-0.13930959]\n",
      " [ 0.31470534]]\n",
      "77 Cost:  0.15776408 \n",
      "Prediction:\n",
      " [[ 1.1875173 ]\n",
      " [ 1.7229735 ]\n",
      " [ 0.9794555 ]\n",
      " [ 0.1815759 ]\n",
      " [ 0.60875785]\n",
      " [ 0.6862154 ]\n",
      " [-0.13931486]\n",
      " [ 0.31469968]]\n",
      "78 Cost:  0.1577596 \n",
      "Prediction:\n",
      " [[ 1.1875058 ]\n",
      " [ 1.722961  ]\n",
      " [ 0.97944546]\n",
      " [ 0.18156883]\n",
      " [ 0.60874915]\n",
      " [ 0.68620694]\n",
      " [-0.13932015]\n",
      " [ 0.31469408]]\n",
      "79 Cost:  0.15775508 \n",
      "Prediction:\n",
      " [[ 1.187494  ]\n",
      " [ 1.7229483 ]\n",
      " [ 0.97943556]\n",
      " [ 0.18156189]\n",
      " [ 0.6087403 ]\n",
      " [ 0.68619835]\n",
      " [-0.13932543]\n",
      " [ 0.3146884 ]]\n",
      "80 Cost:  0.1577506 \n",
      "Prediction:\n",
      " [[ 1.1874822 ]\n",
      " [ 1.7229358 ]\n",
      " [ 0.97942555]\n",
      " [ 0.18155482]\n",
      " [ 0.60873175]\n",
      " [ 0.6861899 ]\n",
      " [-0.13933069]\n",
      " [ 0.31468275]]\n",
      "81 Cost:  0.15774614 \n",
      "Prediction:\n",
      " [[ 1.1874706 ]\n",
      " [ 1.7229234 ]\n",
      " [ 0.9794158 ]\n",
      " [ 0.18154782]\n",
      " [ 0.6087229 ]\n",
      " [ 0.6861814 ]\n",
      " [-0.13933595]\n",
      " [ 0.3146771 ]]\n",
      "82 Cost:  0.15774167 \n",
      "Prediction:\n",
      " [[ 1.1874591 ]\n",
      " [ 1.7229109 ]\n",
      " [ 0.97940576]\n",
      " [ 0.18154082]\n",
      " [ 0.6087142 ]\n",
      " [ 0.6861731 ]\n",
      " [-0.13934122]\n",
      " [ 0.3146715 ]]\n",
      "83 Cost:  0.15773717 \n",
      "Prediction:\n",
      " [[ 1.1874473 ]\n",
      " [ 1.7228982 ]\n",
      " [ 0.97939587]\n",
      " [ 0.18153375]\n",
      " [ 0.6087054 ]\n",
      " [ 0.6861646 ]\n",
      " [-0.1393465 ]\n",
      " [ 0.31466582]]\n",
      "84 Cost:  0.15773268 \n",
      "Prediction:\n",
      " [[ 1.1874355 ]\n",
      " [ 1.7228857 ]\n",
      " [ 0.979386  ]\n",
      " [ 0.18152669]\n",
      " [ 0.6086967 ]\n",
      " [ 0.68615615]\n",
      " [-0.13935177]\n",
      " [ 0.31466016]]\n",
      "85 Cost:  0.15772817 \n",
      "Prediction:\n",
      " [[ 1.1874238 ]\n",
      " [ 1.7228731 ]\n",
      " [ 0.97937596]\n",
      " [ 0.18151972]\n",
      " [ 0.608688  ]\n",
      " [ 0.6861477 ]\n",
      " [-0.13935703]\n",
      " [ 0.3146545 ]]\n",
      "86 Cost:  0.1577237 \n",
      "Prediction:\n",
      " [[ 1.1874124 ]\n",
      " [ 1.7228606 ]\n",
      " [ 0.97936606]\n",
      " [ 0.18151268]\n",
      " [ 0.6086793 ]\n",
      " [ 0.6861392 ]\n",
      " [-0.13936232]\n",
      " [ 0.31464884]]\n",
      "87 Cost:  0.15771924 \n",
      "Prediction:\n",
      " [[ 1.1874006 ]\n",
      " [ 1.7228482 ]\n",
      " [ 0.97935605]\n",
      " [ 0.18150565]\n",
      " [ 0.6086706 ]\n",
      " [ 0.6861309 ]\n",
      " [-0.13936758]\n",
      " [ 0.31464323]]\n",
      "88 Cost:  0.15771474 \n",
      "Prediction:\n",
      " [[ 1.1873888 ]\n",
      " [ 1.7228357 ]\n",
      " [ 0.97934616]\n",
      " [ 0.18149868]\n",
      " [ 0.6086618 ]\n",
      " [ 0.6861224 ]\n",
      " [-0.13937286]\n",
      " [ 0.31463757]]\n",
      "89 Cost:  0.15771022 \n",
      "Prediction:\n",
      " [[ 1.1873771 ]\n",
      " [ 1.722823  ]\n",
      " [ 0.97933626]\n",
      " [ 0.18149158]\n",
      " [ 0.60865307]\n",
      " [ 0.68611383]\n",
      " [-0.13937812]\n",
      " [ 0.31463185]]\n",
      "90 Cost:  0.15770577 \n",
      "Prediction:\n",
      " [[ 1.1873657 ]\n",
      " [ 1.7228105 ]\n",
      " [ 0.97932637]\n",
      " [ 0.18148455]\n",
      " [ 0.60864425]\n",
      " [ 0.6861054 ]\n",
      " [-0.13938339]\n",
      " [ 0.31462625]]\n",
      "91 Cost:  0.15770125 \n",
      "Prediction:\n",
      " [[ 1.1873538 ]\n",
      " [ 1.7227979 ]\n",
      " [ 0.97931623]\n",
      " [ 0.18147758]\n",
      " [ 0.60863566]\n",
      " [ 0.686097  ]\n",
      " [-0.13938867]\n",
      " [ 0.31462058]]\n",
      "92 Cost:  0.15769683 \n",
      "Prediction:\n",
      " [[ 1.187342  ]\n",
      " [ 1.7227856 ]\n",
      " [ 0.97930646]\n",
      " [ 0.18147054]\n",
      " [ 0.60862684]\n",
      " [ 0.68608856]\n",
      " [-0.13939394]\n",
      " [ 0.31461498]]\n",
      "93 Cost:  0.15769225 \n",
      "Prediction:\n",
      " [[ 1.1873304 ]\n",
      " [ 1.7227727 ]\n",
      " [ 0.97929657]\n",
      " [ 0.18146345]\n",
      " [ 0.60861814]\n",
      " [ 0.68608   ]\n",
      " [-0.13939922]\n",
      " [ 0.31460926]]\n",
      "94 Cost:  0.15768786 \n",
      "Prediction:\n",
      " [[ 1.1873189 ]\n",
      " [ 1.7227604 ]\n",
      " [ 0.9792867 ]\n",
      " [ 0.18145648]\n",
      " [ 0.60860944]\n",
      " [ 0.68607163]\n",
      " [-0.13940448]\n",
      " [ 0.31460366]]\n",
      "95 Cost:  0.15768333 \n",
      "Prediction:\n",
      " [[ 1.1873071 ]\n",
      " [ 1.7227478 ]\n",
      " [ 0.97927654]\n",
      " [ 0.18144944]\n",
      " [ 0.6086006 ]\n",
      " [ 0.6860632 ]\n",
      " [-0.13940975]\n",
      " [ 0.314598  ]]\n",
      "96 Cost:  0.15767884 \n",
      "Prediction:\n",
      " [[ 1.1872953 ]\n",
      " [ 1.7227353 ]\n",
      " [ 0.97926664]\n",
      " [ 0.18144241]\n",
      " [ 0.6085919 ]\n",
      " [ 0.6860547 ]\n",
      " [-0.13941503]\n",
      " [ 0.3145924 ]]\n",
      "97 Cost:  0.15767434 \n",
      "Prediction:\n",
      " [[ 1.1872836 ]\n",
      " [ 1.7227226 ]\n",
      " [ 0.97925675]\n",
      " [ 0.18143544]\n",
      " [ 0.6085832 ]\n",
      " [ 0.68604636]\n",
      " [-0.1394203 ]\n",
      " [ 0.31458673]]\n",
      "98 Cost:  0.15766987 \n",
      "Prediction:\n",
      " [[ 1.1872722 ]\n",
      " [ 1.7227101 ]\n",
      " [ 0.97924685]\n",
      " [ 0.18142834]\n",
      " [ 0.6085745 ]\n",
      " [ 0.6860378 ]\n",
      " [-0.13942558]\n",
      " [ 0.314581  ]]\n",
      "99 Cost:  0.15766542 \n",
      "Prediction:\n",
      " [[ 1.1872604 ]\n",
      " [ 1.7226977 ]\n",
      " [ 0.97923684]\n",
      " [ 0.18142131]\n",
      " [ 0.6085658 ]\n",
      " [ 0.6860293 ]\n",
      " [-0.13943084]\n",
      " [ 0.3145754 ]]\n",
      "100 Cost:  0.15766093 \n",
      "Prediction:\n",
      " [[ 1.1872486 ]\n",
      " [ 1.7226852 ]\n",
      " [ 0.97922695]\n",
      " [ 0.18141428]\n",
      " [ 0.6085571 ]\n",
      " [ 0.686021  ]\n",
      " [-0.13943611]\n",
      " [ 0.31456974]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.set_random_seed(777) # for reproducibility\n",
    "\n",
    "def min_max_scaler(data): # 정규화\n",
    "    numerator = data - np.min(data,0)\n",
    "    denominator = np.max(data,0) - np.min(data,0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "xy = np.array(\n",
    "    [\n",
    "        [828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "        [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "        [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "        [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "        [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "        [819, 823, 1198100, 816, 820.450012],\n",
    "        [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "        [809.51001, 816.659973, 1398100, 804.539978, 809.559998],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# very important. It does not work without it\n",
    "xy = min_max_scaler(xy)\n",
    "print(xy)\n",
    "\n",
    "x_data = xy[:,0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None,4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4,1]),name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]),name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "hypothesis = tf.matmul(X,W) + b\n",
    "\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-5).minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "with tf.Session() as sess:\n",
    "    # Initializes global variables in the graph.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(101):\n",
    "        _, cost_val, hy_val = sess.run([train,cost,hypothesis],feed_dict={X: x_data, Y:y_data})\n",
    "        \n",
    "        print(step, \"Cost: \",cost_val, \"\\nPrediction:\\n\", hy_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수 만들지 않고 사이킷런 라이브러리 내의 MinMaxScaler() 사용\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "xy = np.array(\n",
    "    [\n",
    "        [828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "        [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "        [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "        [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "        [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "        [819, 823, 1198100, 816, 820.450012],\n",
    "        [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "        [809.51001, 816.659973, 1398100, 804.539978, 809.559998],\n",
    "    ]\n",
    ")\n",
    "\n",
    "xy = MinMaxScaler().fit_transform(xy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lab 07-4 mnist introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSyUoo7UOYR4PgcmG_ZrvyuwIGvGFi5xLBLeQ&usqp=CAU'>  \n",
    "\n",
    "28 * 28 * 1 image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001, Cost: 2.808081151\n",
      "Epoch: 0002, Cost: 1.071686683\n",
      "Epoch: 0003, Cost: 0.858307041\n",
      "Epoch: 0004, Cost: 0.756335031\n",
      "Epoch: 0005, Cost: 0.693019284\n",
      "Epoch: 0006, Cost: 0.647818260\n",
      "Epoch: 0007, Cost: 0.613271528\n",
      "Epoch: 0008, Cost: 0.585457307\n",
      "Epoch: 0009, Cost: 0.561945384\n",
      "Epoch: 0010, Cost: 0.542746392\n",
      "Epoch: 0011, Cost: 0.525466450\n",
      "Epoch: 0012, Cost: 0.510094583\n",
      "Epoch: 0013, Cost: 0.496971939\n",
      "Epoch: 0014, Cost: 0.485629936\n",
      "Epoch: 0015, Cost: 0.474338435\n",
      "Learning finished\n",
      "Accuracy:  0.8905\n",
      "Label:  [1]\n",
      "Prediction:  [1]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "tf.set_random_seed(777) # for reproducibility\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True) # one_hot으로 데이터를 읽어옴\n",
    "\n",
    "nb_classes = 10\n",
    "\n",
    "# MNIST data image of shape 28 * 28 = 784\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "# 0 - 9 digits recognition = 10 classes\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([784, nb_classes]))\n",
    "b = tf.Variable(tf.random_normal([nb_classes]))\n",
    "\n",
    "# Hypothesis (using softmax)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Test model\n",
    "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "# parameters\n",
    "num_epochs = 15\n",
    "batch_size = 100\n",
    "num_iterations = int(mnist.train.num_examples / batch_size) # iteration : 전체 사이즈 개수 / batch size\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Training cycle\n",
    "    for epoch in range(num_epochs):\n",
    "        avg_cost = 0\n",
    "\n",
    "        for i in range(num_iterations):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += cost_val / num_iterations # 각 batch마다 가설을 세우고 cost를 구한 다음 모든 batch의 cost의 평균을 구함\n",
    "\n",
    "        print(\"Epoch: {:04d}, Cost: {:.9f}\".format(epoch + 1, avg_cost))\n",
    "\n",
    "    print(\"Learning finished\")\n",
    "\n",
    "    # Test the model using test sets\n",
    "    print(\n",
    "        \"Accuracy: \",\n",
    "        accuracy.eval( # = sess.run\n",
    "            session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    #-----------------------------------------------------------------------------------#\n",
    "    # Sample image show and prediction\n",
    "    # Get one and predict\n",
    "    r = random.randint(0, mnist.test.num_examples - 1)\n",
    "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r : r + 1], 1)))\n",
    "    print(\n",
    "        \"Prediction: \",\n",
    "        sess.run(tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r : r + 1]}),\n",
    "    )\n",
    "\n",
    "    plt.imshow(\n",
    "        mnist.test.images[r : r + 1].reshape(28, 28),\n",
    "        cmap=\"Greys\",\n",
    "        interpolation=\"nearest\",\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training epoch/batch**  \n",
    "\n",
    "\n",
    "In the neural network terminology:  \n",
    "- one **epoch** = one forward pass and one backward pass of all the training examples \n",
    " (전체 데이터 셋을 한번 돈 것)  \n",
    "- **batch size** = the number of training examples in one forward/backward pass. The higher the batch size, the more memory space you'll need.  \n",
    "- number of **iterations** = number of passes, each pass using [batch size] number of examples. To be clear, one pass = one forward pass + one backward pass(we do not count the forward pass and backward pass as two different passes).\n",
    "\n",
    "\n",
    "<u>Example : if you have 1000 training examples, and your batch size is 500, then it will take 2 iterations to complete 1 epoch.</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist download complete\n",
      "normalization done\n",
      "WARNING:tensorflow:From C:\\Users\\dldms\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 19s 311us/sample - loss: 0.7905 - acc: 0.8067\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 17s 280us/sample - loss: 0.4573 - acc: 0.8802\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 16s 275us/sample - loss: 0.4039 - acc: 0.8907 - loss: 0.4044 - a\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 18s 293us/sample - loss: 0.3771 - acc: 0.8968\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 15s 256us/sample - loss: 0.3603 - acc: 0.9003\n",
      "10000/10000 - 1s - loss: 0.3364 - acc: 0.9094\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.33643503231406213, 0.9094]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 추가\n",
    "import tensorflow as tf\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "print(\"mnist download complete\")\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train/255.0, x_test/255.0\n",
    "print(\"normalization done\")\n",
    "\n",
    "#linear classifier\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28)), #28 by 28 mnist input flatten\n",
    "    tf.keras.layers.Dense(10,activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
